{"pages":[{"title":"About Me","text":"​ 程序员小黄，会吃福建人的广东人，面临90后中年危机，爱的人都叫夏洛特。 Roles 华南理工大学SE本科在读 M$ Club技术部指定背锅位 神(hua)出(shui)鬼(mo)没(yu)校园足球边裁 每次签到都慢过107的宿舍长 Projects赶过死线 云计算大作业 建模课大作业 数据库实训 西加加实训 搞过副业 卡片式日记应用开发 俱乐部门户网站开发 抱过大腿 BBT SRP ACM GAC 互联网+大赛 黑客马拉松 微软空间站 看过风景 拼客学院Linux运维训练营 微软亚研2019技术夏令营 正在背锅 IT项目管理大作业 俱乐部微信程序开发 多剧情分支游戏开发 雅典娜杯数据挖掘大赛 即将生产 软件体系结构大作业 肿瘤化疗助手后端开发 Focus 迷恋云计算，操着开发的心选的大数据方向（非ML狂热粉丝，目前水平止于调包 逃 FullStack爱好者（实际上是全菜工程师，手持跨平台开发、轻量级应用潜力股，用谷歌全家桶武装大脑，后被M$诱惑有叛逃迹象 励志成为优秀的软件工程狮+架构狮，respect计算机科学家，认识超棒的设计狮+产品经理，如果可以想当一位还不错的PM Hobbies 努力为祖国健康打码五十年 诺基亚信仰用户，电子产品云收藏者 左派青年，涉猎文史哲 平权，能出力的出力，能发声的发声 书、歌、剧，赏画有待提高 曾经是一名不合格的剪刀手 热爱小语种，推广文言文 只有篆刻的时候字是允许丑的 FM拜仁Chef Coach，没事踢波 精神健身达人 Links Github Stack Overflow V2EX Zhihu Jianshu Gitee SegmentFault Todos 更新链接 丰富图片 详细介绍","link":"/about/index.html"}],"posts":[{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(0) Introduction","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies Once you complete this course, you will have a good understanding of the terminology, tools, and technologies associated with today’s top cloud platforms, which can help pave the way to a lucrative career in technology. IntroductionConcept*”Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction”.* Terms nested virtualization overcommitting Features Speed and AgilityThe required resources are just one click away, which saves time and provides agility. We can also easily scale up or down, depending on our need. CostIt reduces the up-front cost to set up the infrastructure, and allows us to focus on applications and business. Cloud providers have features to estimate the cost, which helps us plan better. Easy Access to ResourcesAs users, we can access our infrastructure from any place and device, as long as we can connect to the provider. MaintenanceAll the maintenance work for the resources is done by the provider. As end users, we do not have to worry about this aspect. Multi-tenancyMultiple users can use the same pool of resources. ReliabilityResources can be hosted in different data center locations, to provide increased reliability. Category Infrastructure as a Service (IaaS) Platform as a Service (PaaS) Software as a Service (SaaS) Model Private CloudIt is designated and operated solely for one organization. It can be hosted internally or externally and managed by internal teams or a third party. We can build a private cloud using a software stack like OpenStack. Public CloudIt is open to the public and anybody can use it after swiping the credit card. Amazon Web Services and Google Compute Engine are examples of public clouds. Hybrid CloudPublic and private clouds are bound together to offer the hybrid cloud. Among other things, a hybrid cloud can be used to: Store sensitive information on a private cloud, while offering public services based on that information from a public cloud. Meet the temporary resources needed from the public cloud. These temporary resources cannot be met from a private cloud.","link":"/2019/07/11/Introduction to Cloud Infrastructure Technologies(0) Introduction/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(1) Virtualization","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Describe the different types of virtualization. Explain how hypervisors can be used to create virtual machines. Create and configure virtual machines automatically, using KVM, VirtualBox and Vagrant. VirtualizationConcept*”In computing, virtualization refers to the act of creating a virtual (rather than actual) version of something, including virtual computer hardware platforms, operating systems, storage devices, and computer resources”.* KVM VirtualBox VirtualBox is distributed under the GNU General Public License (GPL) version 2. VagrantConcept*”Configuring and sharing one VM is easy, but, when we have to deal with multiple VMs for the same project, doing everything manually can be tiresome. Vagrant by HashiCorp helps us automate the setup of one or more VMs by providing an end-to-end lifecycle using the vagrant command line. Vagrant is a cross-platform tool. It can be installed on Linux, Mac OSX, and Windows. We have to use different providers, depending on the OS. It has recently added support for Docker, which can help us manage Docker containers.“* Features Boxes We need to provide an image in the Vagrantfile, which we can use to instantiate machines. In the example above, we have used centos/7 as the base image. If the image is not available locally, then it can be downloaded from a central repository like Atlas, which is the image repository provided by HashiCorp. We can version these images and use them depending on our need, by updating the Vagrantfile accordingly. Vagrant Providers Providers are the underlying engine/hypervisor used to provision a machine. By default, Vagrant supports VirtualBox, Hyper-V and Docker. We also have custom providers, like KVM, AWS, etc. VirtualBox is the default provider. Synced Folders With the Synced Folder feature, we can sync a directory on the host system with a VM, which helps the user manage shared files/directories easily. For example, in the above example, if we un-comment the line below from Vagrantfile, then the ../data folder from the current working directory of the host system would be shared with the /vagrant_data file on the VM. 1config.vm.synced_folder &quot;../data&quot;, &quot;vagrant_data&quot; ProvisioningProvisioners allow us to automatically install software, make configuration changes, etc. after the machine is booted. It is a part of the vagrant up process. There are many types of provisioners available, such as File, Shell, Ansible, Puppet, Chef, Docker, etc. In the example below, we used Shell as the provisioner to install the vim package. 123config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL yum install vim -ySHELL PluginsWe can use plugins to extend the functionality of Vagrant. VagrantFile VagrantCmd vagrant up: start VM vagrant halt: stop VM vagrant destory: delete VM vagrant status: check status vagrant user: login in as user","link":"/2019/07/11/Introduction to Cloud Infrastructure Technologies(1) Virtualization/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(2) Infrastructure as a Service (IaaS)","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Explain the concept of Infrastructure as a Service (IaaS). Distinguish between different IaaS providers. Provision a virtual machine on top of different IaaS providers. Infrastructure as a Service (IaaS)Concept“Infrastructure as a Service (IaaS) is a form of cloud computing which provides on-demand physical and virtual computing resources, storage, network, firewall, load balancers, etc. To provide virtual computing resources, IaaS uses some form of hypervisor, like Xen, KVM, VMware ESX/ESXi, Hyper-V, etc.ther than VMs, some IaaS providers offer bare metal machines for provisioning”. Amazon EC2Concept*”Amazon Web Services (AWS) is one of the leaders in providing different cloud services. With Amazon Elastic Compute, Amazon provides the IaaS infrastructure, on which most of the other services are built. We can manage compute resources from the Amazon EC2 web interface and can scale up or down, depending on the need. AWS also offers a command line to manage the instances from the command line. Amazon EC2 uses XEN and KVM hypervisors to provision compute resources”.* Features Amazon EC2 offers compute instances for different resources, which we can choose from depending on our need. Amazon EC2 provides some preconfigured images, called Amazon Machine Images (AMIs). These images can be used to quickly start instances. We can also create our own custom AMIs to boot our instances. One important aspect to note is that Amazon supports configuring security and network access to our instances. With Amazon Elastic Block Store (EBS) we can attach/detach persistent storage to our instances. EC2 supports the provisioning of dedicated hosts, which means we can get an entire physical machine for our use. Create an Elastic IP for remapping the Static IP address automatically. Provision a Virtual Private Cloud for isolation. Amazon Virtual Private Cloud provides secure and robust networking for Amazon EC2 instances. Use CloudWatch for monitoring resources and applications. Use Auto Scaling to dynamically resize your resources, etc. Azure Virtual MachineConcept‘“Azure is Microsoft’s cloud offering, which has products in different domains, such as compute, web and mobile, data and storage, Internet of Things, and many others. Through Azure Virtual Machine, Microsoft provides compute provisioning and management: We can manage Virtual Machines from Azure’s web interface. Azure also provides a command line utility to manage resources and applications on the Azure cloud”. Features Azure lets you choose between different tiers, based on the usage and the operating systems or the predefined application virtual machines (SharePoint, Oracle, etc.). Using Resource Manager templates, we can define the template for the virtual machine deployment. Azure offers other features as well, like making seamless hybrid connections, faster I/O in certain types of tiers, backups, etc. Google Compute EngineConcept”Google Cloud Platform is Google’s Cloud offering, which has many products in different domains, like compute, storage, networking, big data, and others. Google Compute Engine provides the compute service. We can manage the instances through GUI, APIs or command line. Access to the individual VM’s console is also available“. Features GCE supports different machine types, which we can choose from depending on our need. GCE has other features as well, like Persistent Disk, Local SSD, Global Load Balancing, Compliance and Security, Automatic Discount, etc. DigitalOceanConcept*”DigitalOcean helps you create a simple cloud quickly, in as little as 55 seconds. All of the VMs are created on top of the KVM hypervisor and have SSD (Solid-State Drive) as the primary disk.”* Features Based on your need, DigitalOcean offers different plans. DigitalOcean provides other features, like Floating IPs, Shared Private Networking, Load Balancers, Team Accounts, etc. It offers a one-click installation of a multitude of application stacks like LAMP, LEMP, MEAN, and Docker. OpenStackConcept*”With OpenStack, we can offer a cloud computing platform for public and private clouds. Other than providing a IaaS solution, OpenStack has evolved over time to provide other services, like Database, Storage, etc”.* FeaturesDue to the modular nature of OpenStack, anyone can add additional components to get specific features or functionality. Some of the major OpenStack components are KeystoneProvides Identity, Token, Catalog, and Policy services to projects. NovaProvides on-demand compute resources. HorizonProvides the Dashboard, which is a web-based user interface to manage the OpenStack service. NeutronImplements the network as a service and provides network capabilities to different OpenStack components. GlanceProvides a service where users can upload and discover data assets, like images and metadata. SwiftProvides a highly available, distributed, eventually consistent object/blob store. CinderProvides block storage as a service. HeatProvides a service to orchestrate composite cloud applications, using a declarative template format through an OpenStack-native REST API. CeilometerIt is part of the Telemetry project and provides data collection services for billing and other purposes. Each of the OpenStack components is also modular by design. For example, with Nova we can select an underneath hypervisor depending on the requirement, which can be either libvirt (qemu/KVM), Hyper-V, VMware, XenServer, Xen via libvirt.","link":"/2019/07/12/Introduction to Cloud Infrastructure Technologies(2) Infrastructure as a Service (IaaS)/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(3) Platform as a Service (PaaS)","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Explain the concept of Platform as a Service (PaaS). Distinguish between different PaaS providers. Deploy an application on top of different PaaS providers: Cloud Foundry, OpenShift, Heroku. Platform as a Service (PaaS)Concept*”Platform as a Service (PaaS) is a class of cloud computing services which allows its users to develop, run, and manage applications without worrying about the underlying infrastructure. With PaaS, users can simply focus on building their applications, which is a great help to developers. We can either use PaaS services offered by different cloud computing providers like Amazon, Google, Azure, etc., or deploy it on-premise, using software like OpenShift Origin. PaaS can be deployed on top of IaaS, or, independently on VMs, bare metal, and containers”.* Cloud FoudryConcept*”Cloud Foundry CF) is an open source Platform as a Service (PaaS) that provides a choice of clouds, developer frameworks, and application services. It can be deployed on-premise or on IaaS, like AWS, vSphere, or OpenStack. There are many commercial CF cloud providers as well, like IBM Cloud Foundry, SAP Cloud Platform, Pivotal Cloud Foundry, etc”.* FeaturesAmong the characteristics employed by Cloud Foundry are the following: Application portability Application auto-scaling Centralized platform management Centralized logging Dynamic routing Application health management Role-based application deployment Horizontal and vertical scaling Security Support for different IaaS technologies. CF Application RuntimeConcept*”CF Application Runtime, previously known as Elastic Runtime, is used by developers to run applications written in any language or framework on the cloud of their choice”.* Features CF Application Runtime uses buildpacks, which provide the framework and runtime support for the applications. They are programming language-specific and have information about how to download dependencies and configure specific applications. Developers can build new buildpacks or customize the existing ones. In addition, by using the Open Service Broker API, we can connect our application to external services like databases or third-party SaaS providers. As presented in the graphic below, the CF Application Runtime platform can manage the entire workflow and lifecycle of the application, from routing to logging. CF Container RuntimeConcept*”CF Container Runtime gives Cloud Foundry the flexibility to deploy developer-built, pre-packaged applications using containers”.* Features BOSH is a cloud-agnostic open source tool for release engineering, deployment, and lifecycle management of complex distributed systems. Kubernetes is a container orchestrator which allows us to deploy containers. The CF Container Runtime platform does it by deploying containers on a Kubernetes cluster, which is managed by CF BOSH. With CF BOSH, you can achieve high availability, scaling, VM healing and upgrades for the Kubernetes cluster. OpenShiftConcept*”OpenShift is an open source PaaS solution provided by Red Hat. It is built on top of the container technology, which uses Kubernetes underneath. OpenShift can be deployed on top of a full-fledged Linux OS or on a Micro OS which is specifically designed to run containers and Kubernetes”.* Features With OpenShift, we can deploy containerized applications. With application images and QuickStart application templates, applications can be deployed with one click. As OpenShift uses Kubernetes, we get all the features offered by Kubernetes, like adding or removing nodes at runtime, persistent storage, auto-scaling, etc. OpenShift has a framework called Source to Image (S2I), which enables us to create container images from the source code repository to deploy applications easily. OpenShift integrates well with Continuous Deployment tools to deploy applications as part of the CI/CD pipeline. With CLI, GUI and IDE integration applications can be managed easily. It enables application portability, meaning that any application created on OpenShift can run on any platform that supports Docker. OpenShift integrated Docker registry, automatic edge load balancing, cluster logging, and integrated metrics. HerokuConcept“Heroku is a fully-managed container-based cloud platform, with integrated data services and a strong ecosystem. Heroku is used to deploy and run modern applications”. Applications should contain the source code, its dependency information and the list of named commands to be executed to deploy it, in a file called Procfile. For each supported language, it has a pre-built image which contains a compiler for that language. This pre-built image is referred to as a buildpack. Multiple buildpacks can be used together. We can also create a custom buildpack. While deploying, we need to send the application’s content to Heroku, either via Git, GitHub, Dropbox or via an API. Once the application is received by Heroku, a buildpack is selected based on the language of preference. To create the runtime which is ready for execution, we compile the application after fetching its dependency and configuration variables on the selected buildpack. This runtime is often referred to as a slug. We can also use third party add-ons to get access to value-added services like logging, caching, monitoring, etc. A combination of slug, configuration variables, and add-ons is referred to as a release, on which we can perform upgrade or rollback. Depending on the process-type declaration in the Procfile, a virtualized UNIX container is created to serve the process in an isolated environment, which can be scaled up or down, based on the requirements. Each virtualized UNIX container is referred to as a dyno. Each dyno gets its own ephemeral storage. Dyno Manager manages dynos across all applications running on Heroku. Features Individual components of an application can be scaled up or down using dynos. It is a very rich ecosystem, and it allows us to extend the functionality of an application with add-ons. Add-ons allow us to easily integrate our applications with other fully-managed cloud services like database, logging, email, etc. Applications can be easily integrated with Salesforce. It enables a development-friendly workflow. It provides the ability to do automated backups.","link":"/2019/07/12/Introduction to Cloud Infrastructure Technologies(3) Platform as a Service (PaaS)/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(5) Micro OSes for Containers","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Discuss the characteristics and functionality of Micro OSes, which are specially designed to run containers. Describe different Micro OSes designed to run containers: Project Atomic and Red Hat CoreOS, VMware Photon, and RancherOS. Deploy containers and containerized applications on Micro OSes. Concept“Once we remove the packages which are not required to boot the base OS and run container-related services, we are left with specialized OSes, which are referred to as Micro OSes for containers”. Atomic HostConcept*”Atomic Host is a lightweight operating system, assembled out of a specific RPM content. It allows us to run just containerized applications in a quick and reliable manner”.* Features Atomic Host can be based on Fedora, CentOS, or Red Hat Enterprise Linux (RHEL). Atomic Host is a sub-project of Project Atomic, which includes other sub-projects, such as Buildah, Cockpit and skopeo. Atomic Host comes out-of-the-box with Kubernetes installed. It also includes several Kubernetes utilities, such as etcd and flannel. ComponentsAtomic Host has a very minimal base OS, but it includes components like systemdand journald to help its users and administrators. It is built on top of the following: rpm-ostreeOne cannot manage individual packages on Atomic Host, as there is no rpm or other related commands. To get any required service, you would have to start a respective container. Atomic Host has two bootable, immutable, and versioned filesystems; one is used to boot the system and the other is used to fetch updates from upstream. rpm-ostree is the tool to manage these two versioned filesystems. systemdIt is used to manage system services for Atomic Host. DockerAtomic Host currently supports Docker as a container runtime. KubernetesWith Kubernetes, we can create a cluster of Atomic Hosts to run applications at scale. Atomic Host also comes with a command called atomic. This command provides a high-level, coherent entry point to the system, and fills in the gaps that are not filled by Linux container implementations, such as upgrading the system to the new rpm-ostree, running containers with pre-defined docker run options using labels, verifying an image, etc. Red Hat CoreOSConcept*”According to the “Bringing CoreOS Technology to Red Hat OpenShift to Deliver a Next-Generation Automated Kubernetes Platform” article posted on the CoreOS blog, Red Hat CoreOS will be based on Fedora and Red Hat Enterprise Linux sources, and is expected to ultimately supersede Atomic Host as Red Hat’s immutable, container-centric operating system”.* Others Atomic Host can be managed using tools such as Cockpit. Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser. VMware PhotonConcept*”Photon OS™ is a technology preview of a minimal Linux container host provided by VMware. It is designed to have a small footprint and boot extremely quickly on VMware platforms”.* Features It supports Docker, rkt, and the Pivotal Garden container specifications. It also has a new, open source, yum-compatible package manager (tdnf). Photon OS™ is a security-hardened Linux. The kernel and other aspects of the Photon OS™ are built with an emphasis on security recommendations given by the Kernel Self-Protection Project (KSPP). It can be easily managed, patched, and updated. It also provides support for persistent volumes to store the data of cloud-native applications on VMware vSAN™ . RancherOSConcept*”RancherOS is a 20 MB Linux distribution that runs Docker containers. It has the least footprint of all the Micro OSes available nowadays. This is possible because it runs directly on top of the Linux kernel. RancherOS is a product provided by Rancher, which is an end-to-end platform used to deploy and run private container services”.* Features It automates OS configuration with cloud-init. It can be customized to add custom system Docker containers using the cloud-init file or Docker Compose. ComponentsRancherOS runs two instances of the Docker daemon. Just after booting, it starts the first instance of the Docker daemon with PID 1 to run system containers like dhcp, udev*,* etc. To run user-level containers, the System Docker daemon creates a service to start other Docker daemons. RancherOS also includes a CLI utility called ros, which can be used to control and configure the system.","link":"/2019/07/15/Introduction to Cloud Infrastructure Technologies(5) Containers Micro OSes for Container/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(8) Microservices","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Explain the concept of microservices. Discuss the benefits and challenges of using microservices. BackgroundOver the years, with different experiments, we evolved towards a new approach, in which a single application is deployed and managed via a small set of services. Each service runs its own process and communicates with other services via lightweight mechanisms like REST APIs. Each of these services is independently deployed and managed. Technologies like containers and unikernels are becoming default choices for creating such services. Concept“Microservices are small, independent processes that communicate with each other to form complex applications which utilize language-agnostic APIs. These services are small building blocks, highly decoupled and focused on doing a small task, facilitating a modular approach tosystem-building. The microservices architectural style is becoming the standard for building continuously deployed systems.” Approaches If you have a complex monolith application, then it is not advisable to rewrite the entire application from scratch. Instead, you should start carving out services from the monolith, which implement the desired functionalities for the code we take out from the monolith. Over time, all or most functionalities will be implemented in the microservices architecture. We can split the monoliths based on the business logic, front-end (presentation), and data access. In the microservices architecture it is recommended to have a local database for individual services. And, if the services need to access the database from other services, then we can implement an event-driven communication between these services. As mentioned earlier, we can split the monolith based on the modules of the monolith application and each time we do it, our monolith shrinks.","link":"/2019/07/17/Introduction to Cloud Infrastructure Technologies(8) Microservices/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(7) Unikernels","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Explain the concept of unikernels. Compare and contrast unikernels and containers. Concept“Unikernels are specialised, single-address-space machine images constructed by using library operating systems. With unikernels, we can also select the part of the kernel needed to run with the specific application. With unikernels, we can create a single address space executable, which has both application and kernel components. The image can be deployed on VMs or bare metal, based on the unikernel’s type.” Categories Specialized and purpose-built unikernelsThey utilize all the modern features of software and hardware, without worrying about the backward compatibility. They are not POSIX-compliant. Some examples of specialized and purpose-built unikernels are ING, HalVM, MirageOS, and Clive. Generalized ‘fat’ unikernels They run unmodified applications, which make them fat. Some examples of generalized ‘fat’ unikernels are BSD Rump kernels, OSv, Drawbridge, etc. ComponentsThe Unikernel goes one step further than other technologies, creating specialized virtual machine images with just: The application code The configuration files of the application The user-space libraries needed by the application The application runtime (like JVM) The system libraries of the unikernel, which allow back and forth communication with the hypervisor. Unikernel images would run directly on top of a hypervisor like Xen or on bare metal, based on the unikernel types. Unikernels and Constainers Both containers and unikernels can co-exist on the same host. They can be managed by the same Docker binary. Unikernels helped Docker to run the Docker Engine on top of Alpine Linux on Mac and Windows with their default hypervisors, which are xhyve Virtual Machine and Hyper-V VM respectively.","link":"/2019/07/17/Introduction to Cloud Infrastructure Technologies(7) Unikernels/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(10) Software-Defined Storage and Storage Management for Containers","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Discuss the basics of Software-Defined Storage. Discuss different storage options with containers using Docker, Kubernetes and Cloud Foundry. Software-Defined StorageConcept“Software-Defined Storage (SDS) is a form of storage virtualization in which the storage hardware is separated from the software which manages it. By doing this, we can bring hardware from different sources and we can manage them with software. Software can provide different features, like replication, erasure coding, snapshot, etc. on top of the pooled resources. Once the pooled storage is configured in the form of a cluster, SDS allows multiple access methods like File, Block, and Object. ” CephConcept“Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability. Ceph is an open source defined and unified storage solution. It can deliver Object, Block and File system storage.” Features It achieves higher throughput by stripping the files/data across the multiple nodes. It achieves adaptive load-balancing by replicating frequently accessed objects over multiple nodes. Components Reliable Autonomic Distributed Object Store (RADOS) It is the object store which stores the objects. This layer makes sure that data is always in a consistent and reliable state. It performs operations like replication, failure detection, recovery, data migration, and rebalancing across the cluster nodes. This layer has the following three major components: Object Storage Device (OSD): This is where the actual user content is written and retrieved with read operations. One OSD daemon is typically tied to one physical disk in the cluster. Ceph Monitors (MON): Monitors are responsible for monitoring the cluster state. All cluster nodes report to Monitors. Monitors map the cluster state through the OSD, Place Groups (PG), CRUSH and Monitor maps. Ceph Metadata Server (MDS): It is needed only by CephFS, to store the file hierarchy and metadata for files. LibradosIt is a library that allows direct access to RADOS from languages like C, C++, Python, Java, PHP, etc. Ceph Block Device, RADOSGW, and CephFS are implemented on top of Librados. Ceph Block DeviceThis provides the block interface for Ceph. It works as a block device and has enterprise features like thin provisioning and snapshots. RADOS Gateway (RADOSGW) This provides a REST API interface for Ceph, which is compatible with Amazon S3 and OpenStack Swift. Ceph File System (CephFS) This provides a POSIX-compliant distributed filesystem on top of Ceph. It relies on Ceph MDS to track the file hierarchy. ArchitectureEverything in Ceph is stored as objects. Ceph uses the CRUSH (Controlled Replication Under Scalable Hashing) algorithm to deterministically find out, write, and read the location of objects. GlusterFSConcept“Gluster is a scalable, distributed file system that aggregates disk storage resources from multiple servers into a single global namespace.” Features GlusterFS does not have a centralized metadata server. It uses an elastic hashing algorithm to store files on bricks. ComponentsThe GlusterFS volume can be accessed using one of the following methods: Native FUSE mount NFS (Network File System) CIFS (Common Internet File System). ArchitecturesTo create shared storage, we need to start by grouping the machines in a trusted pool. Then, we group the directories (called bricks) from those machines in a GlusterFS volume, using FUSE (Filesystem in Userspace). GlusterFS supports different kinds of volumes: distributed GlusterFS volume replicated GlusterFS volume distributed replicated GlusterFS volume dispersed GlusterFS volume distributed dispersed GlusterFS volume. Storage Management for ContainersBackgroundContainers are ephemeral in nature, which means that whatever is stored inside the container would be gone as soon as the container is deleted. It is best practice to store data outside the container, which would be accessible even after the container is gone. In a multi-host environment, containers can be scheduled to run on any host. We need to make sure the data volume required by the container is available on the node on which the container would be running. Docker Volume PluginsBackendsDocker uses copy-on-write to start containers from images, which means we do not have to copy an image while starting a container. All of the changes done by a container are saved in a filesystem layer. Docker images and containers are stored on the host system and we can choose the storage backend for Docker storage, depending on our requirements. Docker supports the following storage backends on Linux: AUFS (Another Union File System) BtrFS Device Mapper Overlay VFS (Virtual File System) ZFS windowsfilter. ComponentsDocker primarily supports two options for storing files on a host system: Docker Volumes On Linux, Docker Volumes are stored under the /var/lib/docker/volumes folder and they are directly managed by Docker. Bind Mounts Using Bind Mounts, Docker can mount any file or directory from the host system into a container. In both cases, Docker bypasses the Union Filesystem, which it uses for copy-on-write. The writes happen directly to the host directory. PluginsOn Linux, Docker also supports tmpfs mounts, which are available in the host system’s memory. Docker also supports third party volume plugins. Some examples of volume plugins are: GlusterFS Blockbridge EMC REX-Ray. Volume plugins are especially helpful when we migrate a stateful container, like a database, on a multi-host environment. In such an environment, we have to make sure that the volume attached to a container is also migrated to the host where the container is migrated or started afresh. CommandsTo create a container with volume, we can use either the docker container run or thedocker container create commands, like the following: 1$ docker container run -d --name web -v /webapp nkhare/myapp The above command would create a Docker volume inside the Docker working directory (default to /var/lib/docker/) on the host system. We can get the exact path via thedocker container inspect command: 1$ docker container inspect web We can give a specific name to a Docker volume and then use it for different operations. To create a named volume, we can run the following command: 1$ docker volume create --name my-named-volume and then mount it. To do a bind mount, we can run the following command: 1$ docker container run -d --name web -v /mnt/webapp:/webapp nkhare/myapp which would mount the host’s /mnt/webapp directory to /myapp, while starting the container. Volume Management in KubernetesConcept“Kubernetes uses volumes to attach external storage to the Pods. A volume is essentially a directory, backed by a storage medium. The storage medium and its contents are determined by the volume type.” A volume in Kubernetes is attached to a Pod and shared among containers of that Pod. The volume has the same lifetime as the Pod and it can outlive the containers of that Pod, which allows data to be preserved across container restarts. TypesA directory which is mounted inside a Pod is backed by the underlying volume type. A volume type decides the properties of the directory, like size, content, etc. Some of the volume types are: emptyDirAn empty volume is created for the Pod as soon as it is scheduled on a worker node. The life of the volume is tightly coupled with the Pod. If the Pod dies, the content of emptyDir is deleted forever. hostPathWith the hostPath volume type, we can share a directory from the host to the Pod. If the Pod dies, the content of the volume is still available on the host. gcePersistentDiskWith the gcePersistentDisk volume type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStoreWith the awsElasticBlockStore volume type, we can mount an AWS EBS volume into a Pod. azureFile With the azureFile volume type, we can mount a Azure persistent disk into a Pod. nfsWith the nfs volume type, we can mount an NFS share into a Pod. persistentVolumeClaimWith the persistentVolumeClaim type we can attach a persistent volume to a Pod. Persistent VolumesBackgroundIn a typical IT environment, storage is managed by the storage/system administrators. The end-user gets instructions to use the storage and he/she does not have to worry about the underlying storage management. Static controlIn the containerized world, we would like to follow similar rules, but it becomes very challenging given the many volume types we have seen earlier. In Kubernetes, this problem is solved with the persistent volume subsystem, which provides APIs to manage and consume the storage. To manage the volume it uses the PersistentVolume (PV) resource type and to consume it, it uses the PersistentVolumeClaim (PVC) resource type. Persistent volumes can be provisioned manually or dynamically. For example, a Kubernetes Administrator has created a few PVs upfront. A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PV resources based on size, access modes, etc. Once a suitable PV is found, it is bound to PVC. After a successful bound, PVC can be used in a Pod. Once a user is done with work, the attached PVC can be released. Then, the underlying PV can be reclaimed and recycled for future usage. Dynamic ControlFor dynamic provisioning of PVs, Kubernetes uses the StorageClass resource, which contains pre-defined provisioners and parameters for the PV creation. With PersistentVolumeClaim (PVC), a user sends the requests for dynamic PV creation, which gets wired to the StorageClass resource. Some of the volume types that support managing storage using PersistentVolume are: GCEPersistentDisk AWSElasticBlockStore AzureFile NFS iSCSI. Cloud Foundry Volume ServiceConcept“On Cloud Foundry, applications connect to other services via a service marketplace. Each service has a service broker, which encapsulates the logic for creating, managing, and binding services to applications. With volume services, the volume service broker allows Cloud Foundry applications to attach external storage.” ComponentsWith the volume bind command, the service broker issues a volume mount instruction which instructs the Diego scheduler to schedule the app instances on cells which have the appropriate volume driver. In the backend, the volume driver gets attached to the device. Cells then mount the device into the container and start the app instance. Following are some examples of CF Volume Services: Local-volume-releaseIt provides local-volume service for Cloud Foundry applications. nfs-volume-releaseThis allows easy mounting of external NFS shares for Cloud Foundry applications. Cephfs-bosh-releaseIt is an open source shared volumes service, built on top of Ceph. Efs-volume-releaseIt provides driver and service broker for provisioning and mounting Amazon EFS volumes. Container Storage Interface (CSI)Concept“For a storage vendor, it is difficult to manage different volume plugins for different orchestrators. Storage vendors and community members from different orchestrators are working together to standardize the volume interface to avoid duplicate work. This is being referred to as the Container Storage Interface (CSI). With CSI, the same volume plugin would work for different container Oorchestrators.”","link":"/2019/07/19/Introduction to Cloud Infrastructure Technologies(10) Software-Defined Storage and Storage Management for Containers/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(12) Tools for Cloud Infrastructure I (Configuration Management)","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: List tools used to configure and manage the Cloud Infrastructure. Discuss and use configuration management tools: Ansible, Puppet, Chef and Salt Open. ConceptInfrastructure as Code“When we have numerous systems (both Physical and VMs) to manage in different environments like Development, QA and Production, we want to do it in an automated way. At any point in time, we want to have consistent and desired state of systems and software installed on them. This is also referred as Infrastructure as Code.” Configuration Management“Configuration Management tools allow us to define the desired state of the systems in an automated way. In this section, we will take a look at some of the Configuration Management tools available today, such as Ansible, Chef, Puppet, and Salt.” AnsibleConcept”Red Hat’s Ansible is an easy-to-use, open source configuration management tool. It is an agentless tool which works on top of SSH. Ansible also automates cloud provisioning, application deployment, orchestration, etc.“ Features It is an agentless tool. It provides role-based access control. ComponentsNodesThe nodes can be grouped together as shown in the picture below. Ansible also supports dynamic inventory files for cloud providers like AWS and OpenStack. The management node connects to these nodes with a password or it can do passwordless login, using SSH keys. The Ansible management node connects to the nodes mentioned in the inventory file and runs the tasks mentioned in the playbook. A management node can be installed on any Unix-based system like Linux, Mac OS X, etc. It can manage any node which supports SSH and Python 2.4 or later. PlaybooksPlaybooks are Ansible’s configuration, deployment, and orchestration language. Below we provide an example of a playbook which performs different tasks based on roles: 123456789101112131415161718192021222324252627282930313233343536373839404142434445---# This playbook deploys the whole application stack in this site. - name: apply common configuration to all nodes hosts: all remote_user: root roles: - common- name: configure and deploy the webservers and application code hosts: webservers remote_user: root roles: - web- name: deploy MySQL and configure the databases hosts: dbservers remote_user: root roles: - dbThe sample tasks mentioned in the playbook are:---# These tasks install http and the php modules.- name: Install http and php etc yum: name={{ item }} state=present with_items: - httpd - php - php-mysql - git - libsemanage-python - libselinux-python- name: insert iptables rule for httpd lineinfile: dest=/etc/sysconfig/iptables create=yes state=present regexp=\"{{ httpd_port }}\" insertafter=\"^:OUTPUT \" line=\"-A INPUT -p tcp --dport {{ httpd_port }} -j ACCEPT\" notify: restart iptables- name: http service state service: name=httpd state=started enabled=yes Ansible ships with a default set of modules, like packaging, network, etc., which can be executed directly or via Playbooks. We can also create custom modules. GalaxyAnsible Galaxy is a free site for finding, downloading, and sharing community-developed Ansible roles. TowerAnsible also has an enterprise product called Ansible Tower, which has a GUI interface, access control, central management, etc. PuppetConcept”Puppet is an open source configuration management tool. It mostly uses the agent/master (client/server) model to configure the systems. The agent is referred to as the Puppet Agent and the master is referred to as the Puppet Master. The Puppet Agent can also work locally and is then referred to as Puppet Apply.“ Features It provides scalability, automation, and centralized reporting. It provides role-based access control. ComponentsPuppet AgentWe need to install Puppet Agent on each system we want to manage/configure with Puppet. Each agent: Connects securely to the Puppet Master to get the series of instructions in a file referred to as the Catalog File. Performs operations from the Catalog File to get to the desired state. Sends back the status to the Puppet Master. Puppet Agent can be installed on the following platforms: Linux Windows Mac OSX. Puppet MasterPuppet Master can be installed only on Unix-based system. It: Compiles the Catalog File for hosts based on the system, configuration, manifest file, etc. Sends the Catalog File to agents when they query the master. Has information about the entire environment, such as host information, metadata (like authentication keys), etc. Gathers reports from each agent and then prepares the overall report. The Catalog FilePuppet prepares a Catalog File based on the manifest file. A manifest file is created using the Puppet Code: 1234567user { 'nkhare': ensure =&gt; present, uid =&gt; '1001', gid =&gt; '1001', shell =&gt; '/bin/bash', home =&gt; '/home/nkhare'} A manifest file can have one or more sections of code, like we exemplified above, and each of these sections of code can have a signature like the following: 1234resource_type { 'resource_name': attribute =&gt; value ...} Puppet defines resources on a system as Type which can be file, user, package, service, etc. They are well-documented in their documentation. After processing the manifest file, Puppet Master prepares the Catalog File based on the target platform. Others Centralized reporting (PuppetDB), which helps us generate reports, search a system, etc. Live system management Puppet Forge, which has ready-to-use modules for manifest files from the community. ChefConcept”Chef uses the client/server model to do the configuration management. The client is installed on each host which we want to manage and is referred to as Chef Client. The server is referred to as Chef Server.“ Features It provides real-time visibility with Chef Analytics. It provides role-based access control. ComponentsAdditionally, there is another component called Chef Workstation, which is used to: Develop cookbooks and recipes. Synchronize chef-repo with the version control system. Run command line tools. Configure policy, roles, etc. Interact with nodes to do a one-off configuration. Chef supports the following platforms for Chef Client: Unix-based systems Mac OS X Windows Cisco IOS XR and NX-OS. Chef Server is supported on the following platforms: Red Hat Enterprise Linux Ubuntu Linux. Chef also has a GUI built on top of Chef Server, which can help ups running the cookbook from the browser, prepare reports, etc. Chef CookbookA Chef Cookbook is the basic unit of configuration which defines a scenario and contains everything that supports the scenario. Two of its most important components are: RecipesA recipe is the most fundamental unit for configuration, which mostly contains resources, resource names, attribute-value pairs, and actions. 1234567package \"apache2\" do action :installendservice \"apache2\" do action [:enable, :start]end AttributesAn attribute helps us define the state of the node. After each chef-client run, the node’s state is updated on the Chef Server. Chef KnifeKnife provides an interface between a local chef-repo and the Chef Server. Salt OpenConcept”Salt Open is an open source configuration management system built on top of a remote execution framework. It uses the client/server model, where the server sends commands and configurations to all the clients in a parallel manner, which the clients run, returning back the status.“ Features It supports agent and agentless deployments. It provides role-based access control. ComponentsSalt MinionsEach client is referred to as a Salt minion. Minions can be installed on: Unix-based systems Windows Mac OS X. Salt MastersA server is referred to as a Salt master. Multi-master is also supported. In a default setup, the Salt master and minions communicate over a high speed data bus, ZeroMQ, which requires an agent to be installed on each minion. Salt also supports an agentless setup on top of SSH. The Salt master and minions communicate over a secure and encrypted channel. Modules and ReturnersRemote execution is based on Modules and Returners. Modules provide basic functionality, like installing packages, managing files, managing containers, etc. All support modules are listed in the Salt documentation. We can also write custom modules. With Returners, we can save a minion’s response on the master or other locations. We can use default Returners or write custom ones. Salt has different State Modules to manage a state. Grains and Pillar DataAll the information collected from minions is saved on the master. The collected information is referred to as Grains. Private information like cryptographic keys and other specific information about each minion which the master has is referred to as Pillar Data. Pillar Data is shared between the master and the individual minion. By combining Grains and Pillar Data, the master can target specific minions to run commands on them.","link":"/2019/07/21/Introduction to Cloud Infrastructure Technologies(12) Tools for Cloud Infrastructure I (Configuration Management)/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(4) Containers","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Discuss about containers and their runtimes. Describe the basic Docker operations. Understand how Project Moby helps create container platforms like Docker. Concept*”Operating-System-level virtualization allows us to run multiple isolated user-space instances in parallel. These user-space instances have the application code, the required libraries, and the required runtime to run the application without any external dependencies. These user-space instances are referred to as containers“.* TermsImages In the container world, this box (containing our application and all its dependencies) is referred to as an image. An image contains the application, its dependencies and the user-space libraries. User-space libraries like glibc enable switching from the user-space to the kernel-space. An image does not contain any kernel-space components. Container A running instance of this box is referred to as a container. We can spin multiple containers from the same image. When a container is created from an image, it runs as a process on the host’s kernel. It is the host kernel’s job to isolate and provide resources to each container. Building BlocksNamespacesA namespace wraps a particular global system resource like network, process IDs in an abstraction, that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. The following global resources are namespaced: pid - provides each namespace to have the same PIDs. Each container has its own PID 1. net - allows each namespace to have its network stack. Each container has its own IP address. mnt - allows each namespace to have its own view of the filesystem hierarchy. ipc - allows each namespace to have its own interprocess communication. uts - allows each namespace to have its own hostname and domainname. user - allows each namespace to have its own user and group ID number spaces. A rootuser inside a container is not the root user of the host on which the container is running. cgroupsControl groups are used to organize processes hierarchically and distribute system resources along the hierarchy in a controlled and configurable manner. The following cgroups are available for Linux: blkio cpu cpuacct cpuset devices freezer memory. Union filesystemThe Union filesystem allows files and directories of separate filesystems, known as layers, to be transparently overlaid on each other, to create a new virtual filesystem. An image used in Docker is made of multiple layers and, while starting a new container, we merge all those layers to create a read-only filesystem. On top of a read-only filesystem, a container gets a read-write layer, which is an ephemeral layer and it is local to the container. RuntimesrunCOver the past few years, we have seen a rapid growth in the interest and adoption for container technologies. Most of the cloud providers and IT vendors offer support for containers. To make sure there is no vendor locking and no inclination towards a particular company or project, IT companies came together and formed an open governance structure, called The Open Container Initiative, under the auspices of The Linux Foundation. The governance body came up with specifications to create standards on Operating System process and application containers. runC is the CLI tool for spawning and running containers according to these specifications. containerdcontainerd is an Open Container Initiative (OCI)-compliant container runtime with an emphasis on simplicity, robustness and portability. It runs as a daemon and manages the entire lifecycle of containers. It is available on Linux and Windows. Docker, which is a containerization platform, uses containerd as a container runtime to manage runC containers. rktrkt (pronounced “rock-it”) is an open source, Apache 2.0-licensed project from CoreOS. It implements the App Container specification. CRI-OCRI-O is an OCI-compatible runtime, which is an implementation of the Kubernetes Container Runtime Interface (CRI). It is a lightweight alternative to using Docker as the runtime for Kubernetes. Project MobyProblemFrom the user perspective, we usually get a seamless experience, irrespective of the underlying platform. However, behind the scenes, someone connects different components like containers runtime, networking, storage, etc. to ensure this high quality experience. Open source projects like containerd and libnetwork are part of the container platform and have their own release cycle, governing model, etc. So, how can we take those individual components and build a container platform like Docker? SolutionProject Moby is the answer. It is an open source project which provides a framework for assembling different container systems to build a container platform like Docker. Individual container systems provide features like image, container, secret management, etc. ApplicationMoby is particularly useful if you want to build your container-based system or just want to experiment with the latest container technologies. It is not recommended for application developers and newbies who are looking for an easy way to run containers. LinuxKit, which is a tool to build minimal Linux distributions to run containers, uses Moby. You can find a few examples at its GitHub repository. DockerConcept*”Docker, Inc. is a company which provides Docker Containerization Platform to run applications using containers. Docker has a client-server architecture, in which a Docker client connects to a server (Docker Host) and executes the commands”.* CommandsYou can find a list of basic Docker operations below: Check version: 1$ docker version About daemon: 1$ docker info List images: 1$ docker image ls Pulling an alpine image: 1$ docker image pull alpine: latest Delete an image: 1$ docker container rm &lt;image id/name&gt; Run a container from a locally-available image: 1$ docker container run -it alpine sh Run a container in the background (-d option) from an image: 1$ docker container run -d --name web nginx List only running containers: 1$ docker container ls List all containers: 1$ docker container ls -a Inject a process inside a running container: 1$ docker container exec -it &lt;container_id/name&gt; bash Stop a container: 1$ docker container stop &lt;container id/name&gt; Delete a container: 1$ docker container rm &lt;container id/name&gt; Containers vs VMs A virtual machine runs on top of a hypervisor, which emulates different hardware, like CPU, memory, etc., so that a guest OS can be installed on top of them. Different kinds of guest OSes can run on top of one hypervisor. Between an application running inside a guest OS and in the outside world, there are multiple layers: the guest OS, the hypervisor, and the host OS. On the other hand, containers run directly as a process on top of the Host OS. There is no indirection as we see in VMs, which help containers to get near-native performance. Also, as the containers have very little footprint, we can pack a higher number of containers than VMs on the same physical machine. As containers run on the host OS, we need to make sure containers are compatible with the host OS.","link":"/2019/07/14/Introduction to Cloud Infrastructure Technologies(4) Containers/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(11) DevOps and CI/CD","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Explain the concept of DevOps. Discuss about Continuous Integration and Continuous Deployment. Run automated tests using tools like Jenkins, Travis CI, Shippable, and Concourse. Understand Cloud Native CI/CD. BackgroundEvery industry thrives for better quality and faster innovation. The IT industry is no exception and has to address numerous challenges, like the following: It must quickly go from business idea to market. It must lower the failure rate for new releases. It must have a shorter lead time between fixes. It must have a faster mean time to recovery. Over the last decade or so, we gradually shifted from the Waterfall model to Agile software development, in which teams deliver working software in smaller and more frequent increments. In this process, the IT operations (Ops) teams were unintentionally left behind, which put a lot of pressure on them, due to high end-to-end deployment rates. By putting the Ops teams in the loop from the very beginning of the development cycle, we can reduce this burn-out and can take advantage of their expertise in the continuous integration process. ConceptDevOps“The collaborative work between Developers and Operations is referred to as DevOps. DevOps is more of a mindset, a way of thinking, versus a set of processes implemented in a specific way.” CI/CD“Besides Continuous Integration (CI), DevOps also enables Continuous Deployment(CD), which can be seen as the next step of CI. In CD, we deploy the entire application/software automatically, provided that all the tests’ results and conditions have met the expectations.” JenkinsConcept“Jenkins is one of the most popular and used tools for doing any kind of automation. It is an open source automation system which can provide Continuous Integration and Continuous Deployment. It is written in Java.” Features Jenkins can build Freestyle, Apache Ant and Apache Maven-based projects. We can also extend the functionality of Jenkins, using plugins. Currently, Jenkins supports more than 1000 plugins in different categories, like Source Code Management, Slave Launchers, Build tools, External tools/site integration, etc. Jenkins also has the functionality to build a pipeline, which allows us to to define an entire application lifecycle. Pipeline is most useful for doing Continuous Deployment. Durable: Pipelines can survive both planned and unplanned restarts of your Jenkins master. Pausable: Pipelines can optionally stop and wait for human input or approval before completing the jobs for which they were built. Versatile: Pipelines support complex real-world CD requirements, including the ability to fork or join, loop, and work in parallel with each other. Extensible: The Pipeline plugin supports custom extensions to its DSL (domain scripting language) and multiple options for integration with other plugins. Travis CIConcept“Travis CI is a hosted, distributed CI solution for projects hosted only on GitHub. ” Features Travis CI supports different databases, like MYSQL, RIAK, memcached, etc. We can also use Docker during the build. Travis CI supports most languages. To see a detailed list of the languages supported, please take a look at the “Language-specific Guides“ page. After running the test, we can deploy the application in many cloud providers, such as Heroku, AWS Codedeploy, Cloud Foundry, OpenShift, etc. A detailed list of providers is available in the “Supported Providers“ page by Travis CI. CommandsTo run the test with CI, first we have to link our GitHub account with Travis and select the project (repository) for which we want to run the test. In the project’s repository, we have to create a .travis.yml file, which defines how our build should be executed step-by-step. A typical build with Travis consists of two steps: install: to install any dependency or pre-requisite. script: to run the build script. We can also add other optional steps, including the deployment steps. Following are all the build options one can put in a .travis.yml file. 123456789before_installinstallbefore_scriptscriptafter_success or after_failure OPTIONAL before_deployOPTIONAL deployOPTIONAL after_deployafter_script ShippableConcept“Shippable is a DevOps Assembly Lines Platform that helps developers and DevOps teams achieve CI/CD and make software releases frequent, predictable, and error-free. We do this by connecting all your DevOps tools and activities into a event-driven, stateful workflow” Features One can use Shippable as a SaaS service, install on-premise using Shippable Server or attach their own servers to Shippable Subscription. Shippable runs all the builds inside a Docker container, which are called minions. Shippable has minions for all the different combinations of programming languages and versions they support. But, if you are already doing development with Docker, you can either use your own image or build a new one, while doing the CI. Currently, Shippable supports the following programming languages for CI: C/C++ Clojure Go Java Node.js PHP Python Ruby Scala. Shippable integrates well with all popular CI/CD and DevOps tools like GitHub, Bitbucket, Docker Hub, JUnit, Kubernetes, Slack, Terraform, etc. CommandsTo run CI tests with Shippable, we have to create a configuration file inside the project’s source code repository which we want to test, called shippable.yml. 12345678910111213141516171819202122232425262728293031323334353637383940414243resources: - name: docs_repo type: gitRepo integration: ric03uec-github pointer: sourceName: shippable/docs branch: master - name: aws_rc_cli type: cliConfig integration: aws_rc_access pointer: region: us-east-1 - name: aws_prod_cli type: cliConfig integration: aws_prod_access pointer: region: us-west-2 jobs: - name: publish_prod_docs type: runSh steps: - IN: docs_repo switch: off - IN: aws_prod_cli - TASK: - script: | pushd $(shipctl get_resource_state \"docs_repo\") ./deployDocs.sh s3://docs.shippable.com us-west-2 production popd - name: publish_rc_docs type: runSh steps: - IN: docs_repo - IN: aws_rc_cli - TASK: - script: | pushd $(shipctl get_resource_state \"docs_repo\") ./deployDocs.sh s3://rcdocs.shippable.com us-east-1 rc popd ConcourseConcept“Concourse is an open source CI/CD system. With Concourse, we run series of tasks to perform desired operations. Each task runs inside a container. Using series of tasks along with resources, we can can build a job or pipeline.” Features In Concourse, the necessary data to run the pipeline can be provided by resources. These resources never affect the performance of a worker. CommandsFollowing is a sample task file: 123456789platform: linuximage_resource: type: docker-image source: {repository: busybox}run: path: echo args: [hello world] Concourse is primarily driven via a CLI, which is referred to as fly. We can use fly to login to our Concourse setup, execute tasks, etc. Cloud Native CI/CDBackgroundSo far, we have seen that containers are now playing a major role in an application’s lifecycle, from packaging to deployment. Containers have brought Dev, QA and Ops teams together, which led to a significant improvement in CI/CD. We can code our CI/CD pipeline and put it along the source code, like we’ve seen in the earlier sections. We have also seen running containers at scale using container orchestrators. There are many options for orchestrators, but Kubernetes seems to be the preferred one. Concept“In the Cloud Native approach, we design a package and run applications on top of our infrastructure (on-premise or cloud) using operations tools like containers, container orchestration and services like continuous integration, logging, monitoring, etc. Kubernetes along with the tooling around it meets our requirements to run Cloud Native applications.” ComponentsLet’s now look at some of the tools which can help us in doing CI/CD with Kubernetes for Cloud Native applications. Helm It is the package manager for Kubernetes. Packages are referred as Charts. Using Helm, we can package, share, install or upgrade an application. It was recently incubated as a CNCF project. Draft It is being promoted as a developer tool cloud native application on Kubernetes. With Draft, we can containerize and deploy an application on Kubernetes. Skaffold It is a tool from Google that helps us build and deploy the code to the Kubernetes development environment each time the code changes locally. It also supports Helm. Argo It is a container-native workflow engine for Kubernetes. Its use cases include running Machine Learning, Data Processing and CI/CD tasks on Kubernetes. Jenkins X Jenkins has been a very popular tool for doing CI/CD and it can be used on Kubernetes as well. But the Jenkins team built a new cloud native CI/CD, Jenkins X from the ground up, to run directly on Kubernetes. Spinnaker It is an open source multi-cloud continuous delivery platform from Netflix for releasing software changes with high velocity. It supports all the major cloud providers like Amazon Web Services, Azure, Google Cloud Platform and OpenStack. It supports Kubernetes natively. GitOps and SecOpsConceptGitOps“With GitOps, it is extended to the entire system. Git becomes the single point of truth for code, deployment configuration, monitoring rules, etc.. With just a Git pull request, we can do/update the complete application deployment. Some of the examples of GitOps are Weave Flux and GitKube.” SecOps“With SecOps, security would not be an afterthought when it comes tp application deployment. It would be included along with the application deployment, to avoid any security risks.”","link":"/2019/07/20/Introduction to Cloud Infrastructure Technologies(11) DevOps and CI_CD/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(9) Software-Defined Networking and Networking for Containers","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Define Software-Defined Networking. Discuss the basics of Software-Defined Networking. Discuss different networking options with containers using Docker, Kubernetes and Cloud Foundry. Software-Defined NetworkingConcept“To connect devices, applications, VMs, and containers, we need a similar level of virtualization in networking. This will allow us to meet the end-to-end business requirements. Software-Defined Networking (SDN) decouples the network control layer from the layer which forwards the traffic. This allows SDN to program the control layer to create custom rules in order to meet the networking requirements.” Features Ingress and egress packetsThese are done at the lowest layer, which decides what to do with ingress packets and which packets to forward, based on forwarding tables. These activities are mapped as Data Plane activities. All routers, switches, modem, etc. are part of this plane. Collect, process, and manage the network informationBy collecting, processing, and managing the network information, the network device makes the forwarding decisions, which the Data Plane follows. These activities are mapped by the Control Plane activities. Some of the protocols which run on the Control Plane are routing and adjacent device discovery. Monitor and manage the networkUsing the tools available in the Management Plane, we can interact with the network device to configure it and monitor it with tools like SNMP (Simple Network Management Protocol). Components Data PlaneThe Data Plane, also called the Forwarding Plane, is responsible for handling data packets and apply actions to them based on rules which we program into lookup-tables. Control Plane The Control Plane is tasked with calculating and programming the actions for the Data Plane. This is where the forwarding decisions are made and where services (e.g. Quality of Service and VLANs) are implemented. Management PlaneThe Management Plane is the place where we can configure, monitor, and manage the network devices. ArchitectureIn Software-Defined Networking, we decouple the Control Plane with the Data Plane. The Control Plane has a centralized view of the overall network, which allows it to create forwarding tables of interest. These tables are then given to the Data Plane to manage network traffic. The Control Plane has well-defined APIs that take requests from applications to configure the network. After preparing the desired state of the network, the Control Plane communicates that to the Data Plane (also known as the Forwarding Plane), using a well-defined protocol like OpenFlow. We can use configuration tools like Ansible or Chef to configure SDN, which brings lots of flexibility and agility on the operations side as well. Networking for ContainersConcept“The host kernel uses the network namespace feature of the Linux kernel to isolate the network from one container to another on the system. Network namespaces can be shared as well.” On a single host, when using the virtual Ethernet (vEth) feature with Linux bridging, we can give a virtual network interface to each container and assign it an IP address. With technologies like Macvlan and IPVlan we can configure each container to have a unique and world-wide routable IP address. If we want to do multi-host networking with containers, the most common solution is to use some form of Overlay network driver, which encapsulates the Layer 2 traffic to a higher layer. Examples of this type of implementation are the Docker Overlay Driver, Flannel, Weave, etc. Project Calico allows multi-host networking on Layer 3 using BGP (Border Gateway Protocol). Standards Container Network Model (CNM) Docker, Inc. is the primary driver for this networking model. It is implemented using the libnetwork project, which has the following utilizations: Null: NOOP implementation of the driver. It is used when no networking is required. Bridge: It provides a Linux-specific bridging implementation based in Linux Bridge. Overlay: It provides a multi-host communication over VXLAN. Remote: It does not provide a driver. Instead, it provides a means of supporting drivers over a remote transport, by which we can write third-party drivers. Container Networking Interface (CNI) It is a Cloud Native Computing Foundation project which consists of specifications and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. It is limited to provide network connectivity of containers and removing allocated resources when the container is deleted. As such, it has a wide range of support. It is used by projects like Kubernetes, OpenShift, Cloud Foundry, etc. Service DiscoveryConcept“Service discovery is a mechanism by which processes and services can find each other automatically and talk. With respect to containers, it is used to map a container name with its IP address, so that we can access the container without worrying about its exact location (IP address). ” Components RegistrationWhen a container starts, the container scheduler registers the mapping in some key-value store like etcd or Consul. And, if the container restarts or stops, then the scheduler updates the mapping accordingly. LookupServices and applications use Lookup to get the address of a container, so that they can connect to it. Generally, this is done using some kind of DNS (Domain Name Server), which is local to the environment. The DNS used resolves the requests by looking at the entries in the key-value store, which is used for Registration. SkyDNS and Mesos-DNS are examples of such DNS services. Listing the Available NetworksCommandsIf we list the available networks after installing the Docker daemon, we should see something like the following: 12345$ docker network lsNETWORK ID NAME DRIVER6f30debc5baf bridge bridgea1798169d2c0 host host4eb0210d40bb none null bridge, null, and host are different network drivers available on a single host. 12345678$ ifconfigdocker0 Link encap:Ethernet HWaddr 02:42:A9:DB:AF:39 inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) BridgeLike the hardware bridge, we can emulate a software bridge on a Linux host. It can forward traffic between two networks based on MAC (hardware address) addresses. By default, Docker creates a docker0 Linux bridge. All the containers on a single host get an IP address from this bridge, unless we specify some other network with the –net= option. Docker uses theLinux’s virtual Ethernet (vEth) feature to create two virtual interfaces, one end of which is attached to the container and the other end to the docker0 bridge. 1234567891011121314$ docker container run -it --name=c1 busybox /bin/sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWE_UP&gt; mtu 65536 qdisc noqueue qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever7: eth0@if8: &lt;BROADCAST,MULTICAST,UP, LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever Inspecting a Bridge Network 1234567891011121314151617181920212223242526272829303132333435363738$ docker network inspect bridge[ { \"Name\": \"bridge\", \"Id\": \"6f30debc5baff467d437e3c7c3de673f21b51f821588aca2e30a7db68f10260c\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\" } ] }, \"Internal\": false, \"Containers\": { \"613f1c7812a9db597e7e0efbd1cc102426edea02d9b281061967e25a4841733f\": { \"Name\": \"c1\", \"EndpointID\": \"80070f69de6d147732eb119e02d161326f40b47a0cc0f7f14ac7d207ac09a695\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\" \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Creating a Bridge Network 12$ docker network create --driver bridge my_bridge$ docker container run --net=my_bridge -itd --name=c2 busybox A bridge network does not support automatic service discovery, so we have to rely on the legacy –link option. NullAs the name suggests, NULL means no networking. If we attach a container to a nulldriver, then it would just get the loopback interface. It would not be accessible from the outside. 12345678$ docker container run -it --name=c3 --net=none busybox /bin/sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 1disc noqueue qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet ::1/128 scope host valid_lft forever preferred_lft forever HostUsing the host driver, we can share the host machine’s network namespace with a container. By doing so, the container would have full access to the host’s network. We can see below that running an ifconfig command inside the container lists all the interfaces of the host system: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ docker container run -it --name=c4 --net=host busybox /bin/sh/ # ifconfigdocker0 Link encap:Ethernet HWaddr 02:42:A9:DB:AF:39 inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:a9ff:fedb:af39/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:536 (536.0 B) TX bytes:648 (648.0 B)eth0 Link encap:Ethernet HWaddr 08:00:27:CA:BD:10 inet addr:10.0.2.15 Bcast:10.0.2.255 Mask:255.255.255.0 inet6 addr: fe80::a00:27ff:feca:bd10/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3399 errors:0 dropped:0 overruns:0 frame:0 TX packets:2050 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1021964 (998.0 KiB) TX bytes:287879 (281.1 KiB)eth1 Link encap:Ethernet HWaddr 08:00:27:00:42:F9 inet addr:192.168.99.100 Bcast:192.168.99.255 Mask:255.255.255.0 inet6 addr: fe80::a00:27ff:fe00:42f9/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:71 errors:0 dropped:0 overruns:0 frame:0 TX packets:46 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:13475 (13.1 KiB) TX bytes:7754 (7.5 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:16 errors:0 dropped:0 overruns:0 frame:0 TX packets:16 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:1021964376 (1.3 KiB) TX bytes:1376 (1.3 KiB)vethb3bb730 Link encap:Ethernet HWaddr 4E:7C:8F:B2:2D:AD inet6 addr: fe80::4c7c:8fff:feb2:2dad/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:16 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:1296 (1.2 KiB) Sharing Network Namespaces Among ContainersCommandsSimilar to host, we can share network namespaces among containers. So, two or more containers can have the same network stack and reach each other by referring to localhost. Let’s take a look at its IP address: 123456789101112131415$ docker container run -it --name=c5 busybox /bin/sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever Now, if we start a new container with the –net=container:CONTAINER option, we can see that the other container has the same IP address. 123456789101112131415$ docker container run -it --name=c6 --net=container:c5 busybox /bin/sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever12: eth0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever Docker Multi-Host NetworkingComponents Docker Overlay DriverWith the Overlay driver, Docker encapsulates the container’s IP packet inside a host’s packet while sending it over the wire. While receiving, Docker on the other host decapsulates the whole packet and forwards the container’s packet to the receiving container. This is accomplished with libnetwork, a built-in VXLAN-based overlay network driver. Macvlan Driver With the Macvlan driver, Docker assigns a MAC (physical) address for each container, and makes it appear as a physical device on the network. As the containers appears in the same physical network as the Docker host, we can assign them an IP from the network subnet as host. As a result, we can do direct container-to-container communication between different hosts. Containers can also directly talk to hosts. We need hardware support to implement the Macvlan driver. For more information about Macvlan, please visit its documentation available on the Docker website. Plugins Weave Network PluginWeave Net provides multi-host container networking for Docker. It also provides service discovery and does not require any external cluster store to save the networking configuration. Weave Net has a Docker Networking Plugin which we can use with Docker deployment. Kuryr Network PluginIt is a part of the OpenStack Kuryr project, which also implements libnetwork’s remote driver API by utilizing Neutron, which is OpenStack’s networking service. In addition, we can write our own driver with Docker remote driver APIs. Kubernetes NetworkingConcept“As we know, the smallest deployment unit in Kubernetes is a Pod, which can have one or more containers. Kubernetes assigns a unique IP address to each Pod. Containers in a Pod share the same network namespace and can refer to each other by localhost. We have seen an example of network namespaces sharing in the previous section. Containers in a Pod can expose unique ports, using which we can connect to individual containers using the same Pod IP.” FeaturesAs each Pod gets a unique IP, Kubernetes assumes that Pods should be able to communicate with each other, irrespective of the nodes they get scheduled on. There are different ways to achieve this. Kubernetes uses the Container Network Interface (CNI) for container networking and has put down the following rules if someone wants to write drivers for Kubernetes networking: All pods can communicate with all other pods without NAT All nodes running pods can communicate with all pods (and vice versa) without NAT The IP that a pod sees itself as is the same IP that other pods see it as. Components FlannelFlannel uses the overlay network, as we have seen with Docker, to meet the Kubernetes networking requirements. CalicoCalico, or Project Calico, uses the BGP protocol to meet the Kubernetes networking requirements. Cloud Foundry: Container-to-Container NetworkingConcept“Container-to-container also allows network policy creation and enforcement using which sets routing rules based on app, destination app, protocol and ports, without going through the Gorouter, a load balancer, or a firewall. ” GorouterBy default, the Gorouter routes the external and internal traffic to different Cloud Foundry components. Even the application- to-application communication happens via the Gorouter. To simplify the app-to-app communication, we can enable container-to-container networking with Cloud Foundry. It implements it using Overlay Networking, which we discussed earlier. With Overlay Networking, each container gets its own unique IP address which is reachable from other application instances.","link":"/2019/07/18/Introduction to Cloud Infrastructure Technologies(9) Software-Defined Networking and Networking for Containers/"},{"title":"\\#Configuration\\# 从零到壹：GitHub Pages + Hexo = Blog","text":"利用GitHub Pages+Hexo打造一个个人博客，主要分为以下五个部分： 环境准备 Environment 文件配置 Configuration 个性化 Customization 博客写作 Writing 双备份 Backup 环境准备 Environment安装Git + Github 安装Git部署插件: 1$ npm install hexo-deployer-git --save 安装Node.js 安装Node.js: Download | Node.js 检查是否安装成功: 12$ node -v$ npm -v 安装Hexo 安装Hexo: 1$ npm install -g hexo-cli 检查是否安装成功: 1$ hexo -v 初始化: 1$ hexo init blog 文件配置 Configuration本地运行123$ hexo clean # 删除缓存$ hexo g # 生成Hexo页面$ hexo s # 本地部署Hexo页面 远程运行123$ hexo clean # 删除缓存$ hexo g # 生成Hexo页面$ hexo d # 远程部署Hexo页面 基本配置/_config.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: LotteWong # 个人博客显示名称subtitle: 在代码符号表象中避难。 # 个人博客副标题description: # 搜索引擎描述信息keywords: # 搜索引擎关键词author: LotteWong # 网站作者avatar: ./themes/icarus/source/images/favicon.ico # 网站头像language: en # 网站语言timezone: Asia/HongKong # 网站时区# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/plugins: # 设置个人博客插件theme: icarus # 设置个人博客主题# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git # 部署类型 repo: git@github.com:LotteWong/lottewong.github.io.git # 部署仓库 branch: master # 部署分支 个性化 Customization/themes/icarus/_config.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234# Version of the Icarus theme that is currently usedversion: 2.3.0# Path or URL to the website's iconfavicon: /images/favicon.ico# Path or URL to RSS atom.xmlrss: # Path or URL to the website's logo to be shown on the left of the navigation bar or footerlogo: /images/logo.png# Open Graph metadata# https://hexo.io/docs/helpers.html#open-graphopen_graph: # Facebook App ID # fb_app_id: # Facebook Admin ID # fb_admins: # Twitter ID # twitter_id: # Twitter site # twitter_site: # Google+ profile link # google_plus: # Navigation bar link settingsnavbar: # Navigation bar menu links menu: Home: / Archives: /archives Categories: /categories Tags: /tags About: /about # Navigation bar links to be shown on the right # links: # Download on GitHub: # icon: fab fa-github # url: 'https://github.com/LotteWong'# Footer section link settingsfooter: # Links to be shown on the right of the footer section links: Github: icon: fab fa-github url: 'https://github.com/LotteWong' RSS: icon: fas fa-rss url: 'https://www.zhihu.com/people/lai-xiu-ping'# Article display settingsarticle: # Code highlight theme # https://github.com/highlightjs/highlight.js/tree/master/src/styles highlight: atom-one-light # Whether to show article thumbnail images thumbnail: true # Whether to show estimate article reading time readtime: true# Search plugin settings# https://ppoffice.github.io/hexo-theme-icarus/categories/Plugins/Searchsearch: # Name of the search plugin type: insight# Comment plugin settings# https://ppoffice.github.io/hexo-theme-icarus/categories/Plugins/Commentcomment: # Name of the comment plugin type: # Donation entries# https://ppoffice.github.io/hexo-theme-icarus/categories/Donation/donate: - # Donation entry name type: alipay # Qrcode image URL qrcode: '/images/alipay.png' - # Donation entry name type: wechat # Qrcode image URL qrcode: '/images/wechat.png' - # Donation entry name type: paypal # Paypal business ID or email address business: 'SuperGsama@outlook.com' # Currency code currency_code: USD - # Donation entry name # type: patreon # URL to the Patreon page # url: ''# Share plugin settings# https://ppoffice.github.io/hexo-theme-icarus/categories/Plugins/Shareshare: # Share plugin name type: # Sidebar settings.# Please be noted that a sidebar is only visible when it has at least one widgetsidebar: # left sidebar settings left: # Whether the left sidebar is sticky when page scrolls # https://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: false # right sidebar settings right: # Whether the right sidebar is sticky when page scrolls # https://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: false# Sidebar widget settings# https://ppoffice.github.io/hexo-theme-icarus/categories/Widgets/widgets: - # Widget name type: profile # Where should the widget be placed, left or right position: left # Author name to be shown in the profile widget author: LotteWong # Title of the author to be shown in the profile widget author_title: SCUT, Undergraduate # Author's current location to be shown in the profile widget location: Guangzhou, China # Path or URL to the avatar to be shown in the profile widget avatar: # Email address for the Gravatar to be shown in the profile widget gravatar: # Whether to show avatar image rounded or square avatar_rounded: false # Path or URL for the follow button follow_link: 'https://www.jianshu.com/u/80ee6b6f3418' # Links to be shown on the bottom of the profile widget social_links: Project: icon: fab fa-creative-commons url: 'https://github.com/scutse-man-month-myth/InkYear' Organization: icon: fab fa-creative-commons-by url: 'https://github.com/scutse-man-month-myth' Developer: icon: fab fa-github url: 'https://github.com/LotteWong' #Facebook: #icon: fab fa-facebook #url: 'https://facebook.com' #Twitter: #icon: fab fa-twitter #url: 'https://twitter.com' #Dribbble: #icon: fab fa-dribbble #url: 'https://dribbble.com' - # Widget name type: toc # Where should the widget be placed, left or right position: left - # Widget name type: links # Where should the widget be placed, left or right position: left # Links to be shown in the links widget links: Dart: 'https://dart.dev/' Flutter: 'https://flutter.dev/' - # Widget name type: category # Where should the widget be placed, left or right position: left - # Widget name type: tagcloud # Where should the widget be placed, left or right position: left - # Widget name type: recent_posts # Where should the widget be placed, left or right position: right - # Widget name type: archive # Where should the widget be placed, left or right position: right - # Widget name type: tag # Where should the widget be placed, left or right position: right# Other plugin settingsplugins: # Enable page animations animejs: true # Enable the lightGallery and Justified Gallery plugins # https://ppoffice.github.io/hexo-theme-icarus/Plugins/General/gallery-plugin/ gallery: true # Enable the Outdated Browser plugin # http://outdatedbrowser.com/ outdated-browser: true # Enable the MathJax plugin # https://ppoffice.github.io/hexo-theme-icarus/Plugins/General/mathjax-plugin/ mathjax: true # Show the back to top button on mobile devices back-to-top: true # Google Analytics plugin settings # https://ppoffice.github.io/hexo-theme-icarus/Plugins/General/site-analytics-plugin/#Google-Analytics google-analytics: # Google Analytics tracking id tracking_id: # Baidu Analytics plugin settings # https://ppoffice.github.io/hexo-theme-icarus/Plugins/General/site-analytics-plugin/#Baidu-Analytics baidu-analytics: # Baidu Analytics tracking id tracking_id: # Hotjar user feedback plugin # https://ppoffice.github.io/hexo-theme-icarus/Plugins/General/site-analytics-plugin/#Hotjar hotjar: # Hotjar site id site_id: # Show a loading progress bar at top of the page progressbar: true # Show the copy button in the highlighted code area clipboard: true # BuSuanZi site/page view counter # https://busuanzi.ibruce.info busuanzi: false# CDN provider settings# https://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/speed-up-your-site-with-custom-cdn/providers: # Name or URL of the JavaScript and/or stylesheet CDN provider cdn: jsdelivr # Name or URL of the webfont CDN provider fontcdn: google # Name or URL of the webfont Icon CDN provider iconcdn: fontawesome 博客写作 Writing 默认 1$ hexo new \"blog title\" 自定义 1234567891011title: {{ blog title }}categories: {{ blog category }}tags:- {{ blog tag }}thumbnail: {{ blog thumbnail }}{{ Abstract }}&lt;!-- more --&gt;{{ Content }} 双备份 Backup Hexo备份: 12# master branch$ hexo d Src备份: 12345# dev branch$ git checkout dev$ git add --all$ git commit -m \"new blog\"$ git push origin dev 待办事项 Todos 对应图标 更多插件 绑定域名 更新外链 参考链接 References GitHub+Hexo 搭建个人网站详细教程 Hexo icarus","link":"/2019/07/10/从零到壹：GitHub Pages + Hexo = Blog/"},{"title":"\\#Cloud Computing\\# Introduction to Cloud Infrastructure Technologies(6) Container Orchestration","text":"Welcome to LFS151x: Introduction to Cloud Infrastructure Technologies By the end of this chapter, you should be able to: Describe different container orchestration tools: Docker Swarm, Kubernetes, Mesos, Nomad, Amazon ECS. Describe different Kubernetes hosted services like AWS Elastic Kubernetes Service (EKS), Azure Kubernetes Services (AKS) and Google Kubernetes Engine (GKE). Deploy sample applications using various container orchestration tools: Docker Swarm, Kubernetes, Mesos, Nomad, Amazon ECS. Concept”Container orchestration is an umbrella term which encompasses container scheduling and cluster management. Container scheduling allows us to decide on which host a container or a group of containers should be deployed. With the cluster management orchestrator we can manage the existing nodes, add or delete nodes, etc.“ Docker SwarmConcept”Docker Swarm is a native container orchestration tool from Docker, Inc. It logically groups multiple Docker engines to create a virtual engine, on which we can deploy and scale applications.“ Features It is compatible with Docker tools and API, so that the existing workflow does not change much. It provides native support to Docker networking and volumes. It can scale up to large numbers of nodes. It supports failover and High Availability for the cluster manager. It uses a declarative approach to define the desired state of the various services of the application stack. For each service, you can declare the number of tasks you want to run. When you scale up or down, the Swarm manager automatically adapts by adding or removing tasks to maintain the desired state. The Docker Swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. The communication between the nodes of Docker Swarm is enforced with Transport Layer Security (TLS), which makes it secure by default. It supports rolling updates, using which we can control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back a task to a previous version of the service. Components Swarm Manager NodesThere can be one or more manager nodes. They accept commands on behalf of the cluster and make scheduling decisions. They also store the cluster state using the Internal Distributed State Store, which uses the Raft consensus algorithm. One or more nodes can be configured as managers, but they work in active/passive modes. Swarm Worker NodesThey run the Docker Engine and the sole purpose of the worker nodes is to run the container workload given by the manager node(s). Docker MachineConcept”Docker Machine helps us configure and manage one or more Docker engines running locally or on cloud environments. With Docker Machine we can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to our host. We can also use Docker Machine to configure a Swarm cluster.“ CommandsDocker Machine has drivers for Amazon EC2, Google Cloud, Digital Ocean, Vagrant, etc., to set up Docker engines. You can also add already running instances of the Docker engine to the Docker Machine: Setting up the Docker engine using the VirtualBox driver: 1$ docker-machine create -d virtualbox dev1 Setting up the Docker engine using DigitalOcean: 1$ docker-machine create --driver digitalocean --digitalocean-access-token=&lt;TOKEN&gt; dev2 Docker ComposeConcept“Docker Compose allows us to define and run multi-container applications through a configuration file. In a configuration file, we can define services, images or Dockerfiles to use, network, etc. Docker Swarm uses Docker Stack to deploy the distributed applications. We can use Docker Compose to generate the stack file, which can be used to deploy applications on Docker Swarm.” DockerFile Docker DatacenterConcept“Docker has another project called Docker Datacenter, which is built on top of UCP and Docker Trusted Registry. Docker Datacenter is hosted completely behind the firewall. With Docker Datacenter we can build an enterprise-class CaaS platform on-premises, as it is built on top of Docker Swarm and integrates well with Docker tools, Docker Registry. It also has other features, such as LDAP/AD integration, monitoring, logging, network and storage plugins.” Docker Enterprise EditionConcept“Docker Enterprise Edition (EE) 2.0 is the Container-as-a-Service (CaaS) platform that manages the entire lifecycle of the applications on enterprise Linux or Windows operating systems and Cloud providers. It supports Docker Swarm and Kubernetes as container orchestrators.” Features It is a multi-Linux, multi-OS, multi-Cloud solution. It supports Docker Swarm and Kubernetes as container orchestrators. It provides centralized cluster management. It has a built-in authentication mechanism with role-based access control (RBAC). Components Docker EE EngineIt is a commercially supported Docker Engine for creating images and running Docker containers. Docker Trusted Registry (DTR) It is a production-grade image registry designed to store images, from Docker, Inc. Universal Control Plane (UCP)It manages the Kubernetes and Swarm orchestrators, deploys applications using the CLI and GUI, and provides High Availability. UCP also provides role-based access control to ensure that only authorized users can make changes and deploy applications to your cluster. KubernetesConcept“Kubernetes is an Apache 2.0-licensed open source project for automating deployment, operations, and scaling of containerized applications. It was started by Google, but many other companies like Docker, Red Hat, and VMware contributed to it. Kubernetes supports container runtimes like Docker, CRI-O, etc., to run containers.” Features It automatically places containers based on resource requirements and other constraints. It supports horizontal scaling through the CLI and GUI. It can auto-scale based on the CPU load as well. It supports rolling updates and rollbacks. It supports multiple volume plugins like the GCP/AWS disk, NFS, iSCSI, Ceph, Gluster, Cinder, Flocker, etc. to attach volumes to pods. It automatically self-heals by restarting failed pods, rescheduling pods from failed nodes, etc. It deploys and updates secrets for an application without rebuilding the image. It supports batch execution. It support High Availability cluster. It eliminates infrastructure lock-in by providing core capabilities for containers without imposing restrictions. We can deploy and update the application at scale. Components ClusterThe cluster is a group of systems (physical or virtual) and other infrastructure resources used by Kubernetes to run containerized applications. Master NodeThe master is a system that takes pod scheduling decisions and manages the replication and manager nodes. It has three main components: API Server, Scheduler, and Controller. There can be more than one master node. Worker NodeA system on which pods are scheduled and run. The node runs a daemon called kubelet to communicate with the master node. kube-proxy, which runs on all nodes, allows applications from the external world. Key-Value StoreThe Kubernetes cluster state is saved in a key-value store, like etcd. It can be either part of the same Kubernetes cluster or it can resides outside. PodThe pod is a co-located group of containers with shared volumes. It is the smallest deployment unit in Kubernetes. A pod can be created independently, but it is recommended to use the Replica Set, even if only a single pod is being deployed. Replica SetThe Replica Set manages the lifecycle of pods. It makes sure that the desired numbers of pods is running at any given point in time. DeploymentsDeployments allow us to provide declarative updates for pods and Replica Sets. We can define Deployments to create new resources, or replace existing ones with new ones. ServiceThe service groups sets of pods together and provides a way to refer to them from a single static IP address and the corresponding DNS name. LabelThe label is an arbitrary key-value pair which is attached to a resource like pod, Replica Set, etc. In the example above, we defined labels as app and tier. SelectorSelectors enable us to group resources based on labels. In the above example, the frontend service will select all pods which have the labels app==dockchat and tier==frontend. VolumeThe volume is an external filesystem or storage which is available to pods. They are built on top of Docker volumes. NamespaceThe namespace allows us to partition the cluster into sub-clusters. KubernetesFileSome typical use cases are presented below: Create a Deployment to bring up a Replica Set and pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the pods (for example, to use a new image). Roll back to an earlier Deployment revision if the current Deployment isn’t stable. Pause and resume a Deployment. Below we provide a sample deployment: Kubernetes Hosted SolutionsConcept“Kubernetes can be deployed anywhere, be it on-premise or on-cloud. If we are deploying on-premise, then our Kubernetes administrators would have to perform all the Kubernetes management tasks like upgrading, back up, etc. With an on-cloud setup, we have different options. For instance, we can manage our own on-premise or opt for hosted Kubernetes services in which all the management tasks would be performed by the service providers.” There are many hosted solutions available for Kubernetes, including: Google Kubernetes EngineOffers managed Kubernetes clusters on Google Cloud Platform. Amazon Elastic Container Service for Kubernetes (Amazon EKS)Offers a managed Kubernetes service on AWS. Azure Kubernetes Service (AKS)Offers a managed Kubernetes clusters Microsoft Azure. Stackpoint.ioProvides Kubernetes infrastructure automation and management for multiple public clouds. OpenShift DedicatedOffers managed Kubernetes clusters powered by Red Hat. Google Kubernetes Engine (GKE)Concept“Google Kubernetes Engine is a fully-managed solution for running Kubernetes on Google Cloud. As we have learned earlier, Kubernetes is used for automating deployment, operations, and scaling of containerized applications. In GKE, Kubernetes can be integrated with all Google Cloud Platform’s services, like Stackdriver monitoring, diagnostics, and logging, identity and access management, etc.” Features It has all of Kubernetes’ features. It runs on a container-optimized OS built and managed by Google. It is a fully-managed service, so the users do not have to worry about managing and scaling the cluster. We can store images privately, using the private container registry. Logging can be enabled easily using Google Cloud Logging. It supports Hybrid Networking to reserve an IP address range for the container cluster. It enables a fast setup of managed clusters. It facilitates increased productivity for Dev and Ops teams. It is Highly Available in multiple zones and SLA promises 99.5% of availability. It has Google-grade managed infrastructure. It can be seamlessly integrated with all GCP services. It provides a feature called Auto Repair, which initiates a repair process for unhealthy nodes. Amazon Elastic Container Service for Kubernetes (EKS)Concept“Amazon Elastic Container Service for Kubernetes is a hosted Kubernetes service offered by AWS. With Amazon EKS, users don’t need to worry about the infrastructure management, deployment and maintenance of the Kubernetes control plane. EKS provides a scalable and highly-available control plane that runs across multiple AWS availability zones. It can automatically detect the unhealthy Kubernetes control plane nodes and replace them. EKS also supports cluster autoscaling, using which it can dynamically add worker nodes, based on the workload. It also integrates with Kubernetes RBAC (Role-Based Control Access) to support AWS IAM authentication.” Features No need to manage the Kubernetes Control Plane. Provides secure communication between the worker nodes and the control plane. Supports auto scaling in response to changes in load. Well integrated with various AWS services, like IAM and CloudTrail. Certified hosted Kubernetes platform. Azure Kubernetes Service (AKS)Concept“Azure Kubernetes Service is a hosted Kubernetes service offered by Microsoft Azure. AKS offers a fully-managed Kubernetes container orchestration service, which reduces the complexity and operational overhead of managing Kubernetes. AKS handles all of the cluster management tasks, health monitoring, upgrades, scaling, etc. AKS also supports cluster autoscaling using which it can dynamically add worker nodes, based on the workload. It supports Kubernetes RBAC (Role-Based Control Access) and can integrate with Azure Active Directory for identity and security management.” Features No need to manage the Kubernetes Control Plane. Supports GUI and CLI-based deployment. Integrates well with other Azure services. Certified hosted Kubernetes platform. Compliant with SOC and ISO/HIPAA/HITRUST. Apache MesosConcept“Apache Mesos was created with this idea in mind, so that we can optimally use the resources available, even if we are running disparate applications on a pool of nodes. It helps us treat a cluster of nodes as one big computer, which manages CPU, memory, and other resources across a cluster. Mesos provides functionality that crosses between Infrastructure as a Service (IaaS) and Platform as a Service (PaaS).” Features It can scale up to 10,000 nodes. It uses ZooKeeper for fault-tolerant replicated master and slaves. It provides support for Docker containers. It enables native isolation between tasks with Linux containers. It allows multi-resource scheduling (memory, CPU, disk, and ports). It uses Java, Python and C++ APIs to develop new parallel applications. It uses WebUI to view cluster statistics. It allows high resource utilization. It helps handling mixed workloads. It provides an easy-to-use container orchestration right out of the box. It ships binaries for different components (e.g. master, slaves, frameworks, etc.), which we can bind together to create our Mesos cluster. Components MasterMaster nodes are the brain of the cluster and provide a single source of truth for running tasks. There is one active master node at any point in time. The master node mediates between schedulers and slaves. Slaves advertise their resources to the master node, then the master node forwards them to the scheduler. Once the scheduler accepts the offer, it sends the task to run on the slave to the master, and the master forwards these tasks to the slave. SlaveSlaves manage resources at each machine level and execute the tasks submitted via the scheduler. FrameworksFrameworks are distributed applications that solve a specific use case. They consist of a scheduler and an executor. The scheduler gets a resource offer, which it can accept or decline. The executor runs the job on the slave, which the scheduler schedules. There are many existing frameworks and we can also create custom ones. Some of the existing frameworks are: Hadoop, Spark, Marathon, Chronos, Jenkins, Aurora, and many more. Executor Executors are used to run jobs on slaves. They can be shell scripts, Docker containers, and programs written in different languages (e.g. Java). Mesosphere DC/OSConcept“Mesosphere Enterprise DC/OS, offers a one-click install and enterprise features like security, monitoring, user interface, etc. on top of Mesos. By default, DC/OS comes with the Marathon framework and others can be added as required.” FeaturesThe Marathon framework has the following features: It starts, stops, scales, and updates applications. It has a nice web interface, API. It is highly available, with no single point of failure. It uses native Docker support. It supports rolling deploy/restart. It allows application health checks. It provides artifact staging. In addition to the Mesos features presented on the previous page, DC/OS provides the following ones: It provides an easy-to-use container orchestration right out of the box. It can configure multiple resource isolation zones. It can support applications with multiple persistent and ephemeral storage options. It allows you to install both public community and private community packaged applications. It allows you to manage your cluster and services using the web and command line interfaces. It allows you to easily scale up and scale down your services. It provides automation for updating services and the systems with zero downtime. Its Enterprise edition provides centralized management, control plane for service availability and performance monitoring. ComponentsThe DC/OS Master has the following default components: Mesos Master ProcessIt is similar to the master component of Mesos. Mesos DNSIt provides service discovery within the cluster, so applications and services running inside the cluster can reach to each other. MarathonIt is a framework which comes by default with DC/OS and provides the init system. ZooKeeperIt is a high-performance coordination service that manages the DC/OS services. Admin RouterIt is an open source Nginx configuration created by Mesosphere, providing central authentication and proxy to DC/OS services within the cluster. The DC/OS Agent nodes have the following components: Mesos Agent ProcessIt runs the mesos-slave process, which is similar to the slave component of Mesos. Mesos ContainerizerIt provides lightweight containerization and resource isolation of executors, using Linux-specific functionality, such as cgroups and namespaces. Docker ContainerIt provides support for launching tasks that contain Docker images. DC/OS has its own command line and web interfaces, and comes with a simple packaging and installation. NomadConcept“HashiCorp Nomad is a cluster manager and resource scheduler from HashiCorp, which is distributed, highly available, and scales to thousands of nodes. It is especially designed to run microservices and batch jobs, and it supports different workloads, like containers (Docker), VMs, and individual applications. It is also capable of scheduling applications and services on different platforms like Linux, Windows and Mac.” Features It handles both cluster management and resource scheduling. It supports multiple workloads, like containers (Docker), VMs, unikernels, and individual applications. It has multi-datacenter and multi-region support. We can have a Nomad client/server running in different clouds, which form the same logical Nomad cluster. It bin-packs applications onto servers to achieve high resource utilization. In Nomad, millions of containers can be deployed or upgraded by using the job file. It provides a built-in dry run execution facility, which shows the scheduling actions that are going to take place. It ensures that applications are running in failure scenarios. It supports long-running services, as well as batch jobs and cron jobs. It provides a built-in mechanism for rolling upgrades. Blue-green and canary deployments can be deployed using a declarative job file syntax. If nodes fail, Nomad automatically redeploys the application from unhealthy nodes to healthy nodes. NomadFileIt is distributed as a single binary, which has all of its dependency and runs in a server and client mode. To submit a job, the user has to define it using a declarative language called HashiCorp Configuration Language (HCL) with its resource requirements. Once submitted, Nomad will find available resources in the cluster and run it to maximize the resource utilization. Amazon ECSConcept“Amazon Elastic Container Service (Amazon ECS) is part of the Amazon Web Services (AWS) offerings. It provides a fast and highly scalable container management service that makes it easy to run, stop, and manage Docker containers on a cluster.” It can be configured in the following two launch modes: Fargate Launch Type AWS Fargate allows us to run containers without managing servers and clusters. In this mode, we just have to package our applications in containers along with CPU, memory, networking and IAM policies. We don’t have to provision, configure, and scale clusters of virtual machines to run containers, as AWS will take care of it for us. EC2 Launch Type With the EC2 launch type, we can provision, patch, and scale the ECS cluster. This gives more control to our servers and provides a range of customization options. Features It is compatible with Docker. It provides a managed cluster, so that users do not have to worry about managing and scaling the cluster. The task definition allows the user to define the applications through a .json file. Shared data volumes, as well as resource constraints for memory and CPU, can also be defined in the same file. It provides APIs to manage clusters, tasks, etc. It allows easy updates of containers to new versions. The monitoring feature is available through AWS CloudWatch. The logging facility is available through AWS CloudTrail. It supports third party Docker Registry or Docker Hub. AWS Fargate allows you to run and manage containers without having to provision or manage servers. AWS ECS allows you to build all types of containers. You can build a long-running service or a batch service in a container and run it on ECS. You can apply your Amazon Virtual Private Cloud (VPC), security groups and AWS Identity and Access Management (IAM) roles to the containers, which helps maintain a secure environment. You can run containers across multiple availability zones within regions to maintain High Availability. ECS can be integrated with AWS services like Elastic Load Balancing, Amazon VPC, AWS IAM, Amazon ECR, AWS Batch, Amazon CloudWatch, AWS CloudFormation, AWS CodeStar, AWS CloudTrail, and more. Components ClusterIt is a logical grouping of tasks or services. With the EC2 launch type, a cluster is also a grouping of container instances. Container InstanceIt is only applicable if we use the EC2 launch type. We define the Amazon EC2 instance to become part of the ECS cluster and to run the container workload. Container AgentIt is only applicable if we use the Fargate launch type. It allows container instances to connect to your cluster. Task DefinitionIt specifies the blueprint of an application, which consists of one or more containers. SchedulerIt places tasks on the cluster. ServiceIt allows one or more instances of tasks to run, depending on the task definition. TaskIt is a running container instance from the task definition. ContainerIt is a Docker container created from the task definition. AmazonEC2File","link":"/2019/07/16/Introduction to Cloud Infrastructure Technologies(6) Container Orchestration/"},{"title":"\\#Fullstack\\# 数据库实训“活在华工”微信小程序 - 后端实现(spring boot + maven)","text":"数据库实训“活在华工”微信小程序 - 后端实现(spring boot + maven) 目录结构 .mvn：maven环境 .setting：project设置 .vscode：launch设置 src：源码文件 target：生成文件 scripts：数据库脚本 mvnw：maven源代码 mvnw.md：maven控制台 .classpath：Java class路径 .factorypath：Java factory路径 .project：Java project设置 pom.xml：maven设置 .gitignore：版本管理忽略文件 README.md：说明文档 LICENSE：开源证书 .vscodelaunch.json12345678910111213141516171819202122232425{ \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"java\", \"name\": \"Debug (Launch) - DBPracticeApplication(demo)\", \"request\": \"launch\", \"cwd\": \"${workspaceFolder}\", \"console\": \"internalConsole\", \"stopOnEntry\": false, \"mainClass\": \"src.main.java.com.example.dbpractice.DBPracticeApplication\", \"args\": \"\" }, { \"type\": \"java\", \"name\": \"Debug (Launch) - DBPracticeApplicationTests(demo)\", \"request\": \"launch\", \"cwd\": \"${workspaceFolder}\", \"console\": \"internalConsole\", \"stopOnEntry\": false, \"mainClass\": \"src.test.java.com.example.dbpractice.DBPracticeApplicationTests\", \"args\": \"\" } ]} scriptsdbpractice.sql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081-- 使用UTF-8编码SET NAMES utf8mb4;-- 创建数据库`dbpractice`CREATE DATABASE `dbpractice`;-- 使用数据库`dbpractice`USE `dbpractice`;-- 检查是否已经存在相应数据表`activity`DROP TABLE IF EXISTS `activity`;-- 使用UTF-8编码SET character_set_client = utf8mb4;-- 创建数据表`activity`CREATE TABLE `activity` ( `id` int(11) NOT NULL AUTO_INCREMENT, `title` varchar(30) NOT NULL, `date` datetime DEFAULT NULL, `place` varchar(50) DEFAULT NULL, `desc` varchar(500) DEFAULT NULL, `sponsor` varchar(15) NOT NULL, `certified` tinyint(1) DEFAULT &apos;0&apos;, PRIMARY KEY (`id`), KEY `sponsor` (`sponsor`), CONSTRAINT `activity_ibfk_1` FOREIGN KEY (`sponsor`) REFERENCES `user` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;-- 向数据表`activity`插入数据LOCK TABLES `activity` WRITE;/* ... */;UNLOCK TABLES;-- 检查是否已经存在相应数据表`participant_activity`DROP TABLE IF EXISTS `participant_activity`;-- 使用UTF-8编码SET character_set_client = utf8mb4;-- 创建数据表`participant_activity`CREATE TABLE `participant_activity` ( `uid` varchar(15) NOT NULL, `aid` int(11) NOT NULL, PRIMARY KEY (`uid`,`aid`), KEY `aid` (`aid`), CONSTRAINT `participant_activity_ibfk_1` FOREIGN KEY (`uid`) REFERENCES `user` (`id`), CONSTRAINT `participant_activity_ibfk_2` FOREIGN KEY (`aid`) REFERENCES `activity` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;-- 向数据表`participant_activity`插入数据LOCK TABLES `participant_activity` WRITE;/* ... */;UNLOCK TABLES;-- 检查是否已经存在相应数据表`user`DROP TABLE IF EXISTS `user`;-- 使用UTF-8编码SET character_set_client = utf8mb4;-- 创建数据表`user`CREATE TABLE `user` ( `id` varchar(15) NOT NULL, `name` varchar(10) NOT NULL, `major` varchar(50) DEFAULT NULL, `address` varchar(50) DEFAULT NULL, `phone` varchar(20) DEFAULT NULL, `email` varchar(20) DEFAULT NULL, `wechat` varchar(20) DEFAULT NULL, `qq` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;-- 向数据表`user`插入数据LOCK TABLES `user` WRITE;/* ... */UNLOCK TABLES; srcmain.java.com.example.dbpractice.config配置层daoDataSourceConfiguration.java12345678910111213141516171819202122232425// Dao层配置之：配置DataSource@Configuration@MapperScan(\"com.example.dbpractice.dao\")public class DataSourceConfiguration { @Value(\"${jdbc.driver}\") private String jdbcDriver; @Value(\"${jdbc.url}\") private String jdbcUrl; @Value(\"${jdbc.username}\") private String jdbcUser; @Value(\"${jdbc.password}\") private String jdbcPassword; @Bean(name = \"dataSource\") public ComboPooledDataSource createDateSource() throws PropertyVetoException { ComboPooledDataSource dataSource=new ComboPooledDataSource(); dataSource.setDriverClass(jdbcDriver); dataSource.setJdbcUrl(jdbcUrl); dataSource.setUser(jdbcUser); dataSource.setPassword(jdbcPassword); // 关闭连接后不自动commit dataSource.setAutoCommitOnClose(false); return dataSource; }} SessionFactoryConfiguration.java1234567891011121314151617181920212223242526272829303132// Dao层配置之：配置SessionFactory@Configurationpublic class SessionFactoryConfiguration { @Value(\"${mybatis_config_file}\") private String mybatisConfigFilePath; @Value(\"${mapper_path}\") private String mapperPath; @Value(\"${entity_package}\") private String entityPackage; @Autowired @Qualifier(\"dataSource\") private DataSource dataSource; @Bean(name = \"sqlSessionFactory\") public SqlSessionFactoryBean createSqlSessionFactoryBean() throws IOException { SqlSessionFactoryBean sqlSessionFactoryBean=new SqlSessionFactoryBean(); // mybatis文件扫描路径 sqlSessionFactoryBean.setConfigLocation(new ClassPathResource(mybatisConfigFilePath)); // mapper文件扫描路径 PathMatchingResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); String packageSearchPath = PathMatchingResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX + mapperPath; sqlSessionFactoryBean.setMapperLocations(resolver.getResources(packageSearchPath)); // entity包扫描路径 sqlSessionFactoryBean.setTypeAliasesPackage(entityPackage); return sqlSessionFactoryBean; // data源扫描路径 sqlSessionFactoryBean.setDataSource(dataSource); }} serviceTransactionManagementConfiguration.java12345678910111213// Service层配置之：配置TransactionManagement@Configuration@EnableTransactionManagementpublic class TransactionManagementConfiguration implements TransactionManagementConfigurer { @Autowired private DataSource dateSource; @Override public PlatformTransactionManager annotationDrivenTransactionManager() { // 启动事务管理机制 return new DataSourceTransactionManager(dateSource); }} main.java.com.example.dbpractice.handler异常层123456789101112@ControllerAdvicepublic class GlobalExceptionHandler { @ExceptionHandler(value = Exception.class) @ResponseBody // 不是返回html而是返回错误信息 private Map&lt;String,Object&gt; exceptionHandler (HttpServletRequest req, Exception e){ Map&lt;String,Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", false); modelMap.put(\"errMsg\", e.getMessage()); return modelMap; }} main.java.com.example.dbpractice.entity实体层（表结构定义）Acitivity.java1234567891011121314151617181920212223242526272829303132333435363738package com.example.dbpractice.entity;import java.util.Date;public class Activity { // 活动id 自增 private Integer id; // 活动标题 private String title; // 活动时间 private Date date; // 活动地点 private String place; // 活动类别 private String tag; // 活动描述 500以内 private String desc; // 活动发起人 uid private String sponsor; // 活动是否官方认证 private Boolean certified; // Constructor public Activity(Integer id, String title, Date date, String place, String tag, String desc, String sponsor, Boolean certified) { this.id = id; this.title = title; this.date = date; this.place = place; this.tag = tag; this.desc = desc; this.sponsor = sponsor; this.certified = certified; } // Getters and setters // ...} Participant_Activity.java1234567891011121314151617package com.example.dbpractice.entity;public class Participant_Activity { // 参与者id private String p_uid; // 参与的活动id private Integer aid; // Constructor public Participant_Activity(String p_uid, Integer aid) { this.p_uid = p_uid; this.aid = aid; } // Getters and setters // ...} User.java12345678910111213141516171819202122232425262728293031package com.example.dbpractice.entity;public class User { // 用户学号 private String uid; // 用户姓名 private String name; // 用户专业 private String major; // 用户地址 private String address; // 用户手机 private String phone; // 用户邮箱 private String email; // 用户微信 private String wechat; // 用户QQ private String qq; // Constructor public User(String uid, String name, String address, String contact) { this.uid=uid; this.name=name; this.address=address; this.contact=contact; } // Getters and setters // ...} main.java.com.example.dbpractice.controller控制层（前后端沟通）DBController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package com.example.dbpractice.controller;import com.example.dbpractice.entity.Activity;import com.example.dbpractice.entity.Participant_Activity;import com.example.dbpractice.entity.User;import com.example.dbpractice.service.DBService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController@RequestMapping(value = \"/dbadmin\")public class DBController { @Autowired private DBService DBService; // 针对user表的可选操作 // 返回用户列表 @RequestMapping(value = \"/listuser\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getUserList(){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); List&lt;User&gt; userList = DBService.getUserList(); modelMap.put(\"userList\",userList); return modelMap; } // 返回uid指定的用户 @RequestMapping(value = \"/getuserbyid\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getUserById(String uid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); User user = DBService.getUserById(uid); modelMap.put(\"user\",user); return modelMap; } // 新增用户 @RequestMapping(value = \"/adduser\",method = RequestMethod.POST) private Map&lt;String,Object&gt; addUser(@RequestBody User userToAdd){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.addUser(userToAdd)); return modelMap; } // 修改用户信息 @RequestMapping(value = \"/modifyuser\",method = RequestMethod.POST) private Map&lt;String,Object&gt; modifyUser(@RequestBody User userToModify){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.modifyUser(userToModify)); return modelMap; } // 删除用户 @RequestMapping(value = \"/deleteuser\",method = RequestMethod.GET) private Map&lt;String,Object&gt; deleteUser(String uid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.deleteUser(uid)); return modelMap; } //针对activity表的可选操作 //返回活动列表 @RequestMapping(value = \"/listactivity\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getActvityList(){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); List&lt;Activity&gt; activityList = DBService.getActivityList(); modelMap.put(\"activityList\",activityList); return modelMap; } // 返回aid指定的活动 @RequestMapping(value = \"/getactivitybyid\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getActivityById(int uid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); Activity activity = DBService.getActivityById(uid); modelMap.put(\"activity\",activity); return modelMap; } // 新增活动 @RequestMapping(value = \"/addactivity\",method = RequestMethod.POST) // TODO: 修改RequestBody为RequestParam以通过小程序POST请求 private Map&lt;String,Object&gt; addUser(@RequestParam Activity activityToAdd){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.addActivity(activityToAdd)); return modelMap; } // 修改活动信息 @RequestMapping(value = \"/modifyactivity\",method = RequestMethod.POST) private Map&lt;String,Object&gt; modifyUser(@RequestBody Activity activityToModify){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.modifyActivity(activityToModify)); return modelMap; } // 删除活动 @RequestMapping(value = \"/deleteactivity\",method = RequestMethod.GET) private Map&lt;String,Object&gt; deleteActivity(int uid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.deleteActivity(uid)); return modelMap; } // 针对participant_activity表的可选操作 // 返回参与关系列表 @RequestMapping(value = \"/listparticipantactivity\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getParticipantActivityList(){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); List&lt;Participant_Activity&gt; paList = DBService.getParticipantActivityList(); modelMap.put(\"paList\",paList); return modelMap; } // 返回uid指定的用户参与的活动 @RequestMapping(value = \"/getactivitybyparticipantid\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getActivityByParticipantId(String uid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); List&lt;Activity&gt; activityList = DBService.getActivityByParticipantId(uid); modelMap.put(\"activityList\",activityList); return modelMap; } // 返回aid指定的活动的参与者 @RequestMapping(value = \"/getparticipantbyactivityid\",method = RequestMethod.GET) private Map&lt;String,Object&gt; getParticipantByActivityId(int aid){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); List&lt;User&gt; userList = DBService.getParticipantByActivityId(aid); modelMap.put(\"userList\",\"userList\"); return modelMap; } // 新增参与关系 @RequestMapping(value = \"/addparticipantactivity\",method = RequestMethod.POST) private Map&lt;String,Object&gt; addUser(@RequestBody Participant_Activity p_aToAdd){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.addParticipantActivity(p_aToAdd)); return modelMap; } //删除参与关系 @RequestMapping(value = \"/deleteparticipantactivity\",method = RequestMethod.GET) private Map&lt;String,Object&gt; deleteParticipantActivity(Participant_Activity p_aToDelete){ Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(\"success\", DBService.deleteParticipantActivity(p_aToDelete)); return modelMap; }} main.java.com.example.dbpractice.handler服务层（表操作封装）DBService.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.example.dbpractice.service;import com.example.dbpractice.entity.Activity;import com.example.dbpractice.entity.Participant_Activity;import com.example.dbpractice.entity.User;import java.util.List;public interface DBService { // activity表 操作 // 查询全部活动 List&lt;Activity&gt; getActivityList(); // 查询全部官方活动 List&lt;Activity&gt; getCertifiedActivity(); // 以aid查询活动 Activity getActivityById(int aid); // 以sponsor的id查询活动 List&lt;Activity&gt; getActivityBySponsorId(int s_uid); // 插入活动记录 boolean addActivity(Activity activity); // 更新活动记录 boolean modifyActivity(Activity activity); // 删除活动记录 boolean deleteActivity(int aid); // participant_activity表 操作 // 返回所有 List&lt;Participant_Activity&gt; getParticipantActivityList(); // 以活动id查询参与者 List&lt;User&gt; getParticipantByActivityId(int aid); // 以参与者id查询活动 List&lt;Activity&gt; getActivityByParticipantId(String p_uid); // 新增参与关系 boolean addParticipantActivity(Participant_Activity p_a); // 删除参与关系 boolean deleteParticipantActivity(Participant_Activity p_a); // user表 操作 // 返回所有 List&lt;User&gt; getUserList(); // 以id查询用户 User getUserById(String uid); // 新增用户 boolean addUser(User user); //返回的int是数据库操作受影响的行数 // 更新用户信息 boolean modifyUser(User user); // 删除用户 boolean deleteUser(String uid);} implDBServiceImpl.java123456789101112131415161718192021222324252627282930313233package com.example.dbpractice.service.impl;import com.example.dbpractice.dao.ActivityDao;import com.example.dbpractice.dao.Participant_ActivityDao;import com.example.dbpractice.dao.UserDao;import com.example.dbpractice.entity.Activity;import com.example.dbpractice.entity.Participant_Activity;import com.example.dbpractice.entity.User;import com.example.dbpractice.service.DBService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class DBServiceImpl implements DBService { @Autowired private UserDao userDao; @Autowired private ActivityDao activityDao; @Autowired private Participant_ActivityDao p_aDao; // activity表操作的实现 // ... // participant_activity表操作的实现 // ... // user表操作的实现 // ...} main.java.com.example.dbpractice.dao接口层（表操作实现）ActivityDao.java123456789101112131415161718192021222324252627package com.example.dbpractice.dao;import com.example.dbpractice.entity.Activity;import java.util.List;public interface ActivityDao { // 新增活动记录 int insertActivity(Activity activity); // 删除活动记录 int deleteActivity(int aid); // 查询全部活动记录 List&lt;Activity&gt; queryActivity(); // 查询官方活动记录 List&lt;Activity&gt; queryCertifiedActivity(); // 以activity的id查询活动记录 Activity queryActivityByActivityId(int aid); // 以sponsor的id查询活动记录 List&lt;Activity&gt; queryActivityBySponsorId(int sid); // TODO: 通过其他字段查询活动记录 // ... // 更新活动记录 int updateActivity(Activity activity); } Participant_ActivityDao.java123456789101112131415161718192021222324252627package com.example.dbpractice.dao;import com.example.dbpractice.entity.Activity;import com.example.dbpractice.entity.Participant_Activity;import com.example.dbpractice.entity.User;import java.util.List;public interface Participant_ActivityDao { // 新增参与关系 int insertParticipantActivity(Participant_Activity p_a); // 删除参与关系 int deleteParticipantActivity(Participant_Activity p_a); // 查询参与关系 List&lt;Participant_Activity&gt; queryParticipantActivity(); // 以参与者的id+活动的id查询参与关系 Participant_Activity queryOneParticipantActivity(Participant_Activity p_a); // 以活动的id查询参与者 List&lt;User&gt; queryParticipantByActivityId(int aid); // 以参与者的id查询活动 List&lt;Activity&gt; queryActivityByParticipantId(String pid); // 更新参与关系 // ...} UserDao.java123456789101112131415161718192021package com.example.dbpractice.dao;import com.example.dbpractice.entity.User;import java.util.List;public interface UserDao { // 新增用户信息 int insertUser(User user); //删除用户信息 int deleteUser(String uid); // 查询用户信息 List&lt;User&gt; queryUser(); // 以用户的id查询用户信息 User queryUserById(String uid); //更新用户信息 int updateUser(User user);} main.resources.Mapper映射层（表操作映射）ActivityDao.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.example.dbpractice.dao.ActivityDao\"&gt; &lt;select id=\"queryActivity\" resultType=\"com.example.dbpractice.entity.Activity\"&gt; SELECT * FROM `activity` &lt;/select&gt; &lt;select id=\"queryCertifiedActivity\" resultType=\"com.example.dbpractice.entity.Activity\"&gt; SELECT * FROM `activity` WHERE `certified`=TRUE &lt;/select&gt; &lt;select id=\"queryActivityById\" resultType=\"com.example.dbpractice.entity.Activity\"&gt; SELECT * FROM `activity` WHERE `id`=#{aid} &lt;/select&gt; &lt;select id=\"queryActivityBySponsorId\" resultType=\"com.example.dbpractice.entity.Activity\"&gt; SELECT `aid` FROM `activity` WHERE `sponsor`=#{s_uid} &lt;/select&gt; &lt;insert id=\"insertActivity\" parameterType=\"com.example.dbpractice.entity.Activity\" useGeneratedKeys=\"true\" keyColumn=\"id\" keyProperty=\"id\"&gt; INSERT INTO `activity` VALUES (#{title},#{date},#{place},#{tag},#{desc},#{sponsor},#{certified}) &lt;/insert&gt; &lt;update id=\"updateActivity\" parameterType=\"com.example.dbpractice.entity.Activity\"&gt; UPDATE `activity` &lt;set&gt; &lt;if test=\"title!=null\"&gt;`title`=#{title}&lt;/if&gt; &lt;if test=\"date!=null\"&gt;`date`=#{date}&lt;/if&gt; &lt;if test=\"place!=null\"&gt;`place`=#{place}&lt;/if&gt; &lt;if test=\"tag!=null\"&gt;`tag`=#{tag}&lt;/if&gt; &lt;if test=\"desc!=null\"&gt;`desc`=#{desc}&lt;/if&gt; &lt;if test=\"sponsor!=null\"&gt;`sponsor`=#{sponsor}&lt;/if&gt; &lt;if test=\"certified!=null\"&gt;`certified`=#{certified}&lt;/if&gt; &lt;/set&gt; WHERE `id`=#{id} &lt;/update&gt; &lt;delete id=\"deleteActivity\"&gt; DELETE FROM `activity` WHERE `id`=#{aid} &lt;/delete&gt;&lt;/mapper&gt; Participant_ActivityDao.xml123456789101112131415161718192021222324252627282930313233&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.example.dbpractice.dao.Participant_ActivityDao\"&gt; &lt;select id=\"queryParticipantActivity\" resultType=\"com.example.dbpractice.entity.Participant_Activity\"&gt; SELECT * FROM `participant_activity` &lt;/select&gt; &lt;select id=\"queryParticipantByActivityId\" resultType=\"com.example.dbpractice.entity.User\"&gt; SELECT `uid` FROM `participant_activity` WHERE `aid`=#{aid} &lt;/select&gt; &lt;select id=\"queryActivityByParticipantId\" resultType=\"com.example.dbpractice.entity.Activity\"&gt; SELECT `aid` FROM `participant_activity` WHERE `uid`=#{p_uid} &lt;/select&gt; &lt;select id=\"queryOneParticipantActivity\" resultType=\"com.example.dbpractice.entity.Participant_Activity\"&gt; SELECT * FROM `participant_activity` WHERE `uid`=#{uid} AND `aid`=#{aid} &lt;/select&gt; &lt;insert id=\"insertParticipantActivity\" parameterType=\"com.example.dbpractice.entity.Participant_Activity\"&gt; INSERT INTO `participant_activity` VALUES (#{p_uid},#{aid}) &lt;/insert&gt; &lt;delete id=\"deleteParticipantActivity\" parameterType=\"com.example.dbpractice.entity.Participant_Activity\"&gt; DELETE FROM `participant_activity` WHERE `uid`=#{p_uid} AND `aid`=#{aid} &lt;/delete&gt;&lt;/mapper&gt; UserDao.xml1234567891011121314151617181920212223242526272829303132&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.example.dbpractice.dao.UserDao\"&gt; &lt;select id=\"queryUser\" resultType=\"com.example.dbpractice.entity.User\" &gt; SELECT `id`,`name`,`address`,`contact` FROM `user` &lt;/select&gt; &lt;select id=\"queryUserById\" resultType=\"com.example.dbpractice.entity.User\"&gt; SELECT `id`,`name`,`address`,`contact` FROM `user` WHERE `id`=#{uid} &lt;/select&gt; &lt;insert id=\"insertUser\" parameterType=\"com.example.dbpractice.entity.User\"&gt; INSERT INTO `user`(`id`,`name`,`address`,`contact`) VALUES(#{uid},#{name},#{address},#{contact}) &lt;/insert&gt; &lt;update id=\"updateUser\" parameterType=\"com.example.dbpractice.entity.User\"&gt; update `user` &lt;set&gt; &lt;if test=\"name!=null\"&gt;`name`=#{name},&lt;/if&gt; &lt;if test=\"address!=null\"&gt;`address`=#{address},&lt;/if&gt; &lt;if test=\"contact!=null\"&gt;`contact`=#{contact}&lt;/if&gt; &lt;/set&gt; where `id`=#{uid} &lt;/update&gt; &lt;delete id=\"deleteUser\"&gt; DELETE FROM `user` WHERE `id`=#{uid} &lt;/delete&gt;&lt;/mapper&gt; application.properties（project配置）1234567891011121314# serverserver.port=8888server.servlet.context-path=/dbpractice# jbbcjdbc.driver=com.mysql.cj.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/dbpractice?useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=UTC&amp;useSSL=falsejdbc.username=rootjdbc.password=password# mybatismybatis_config_file=mybatis-config.xmlmapper_path=/Mapper/*.xmlentity_package=com.example.dbpractice.entity mybatis-config.xml（mybatis配置）1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- Globally enables or disables any caches configured in any mapper under this configuration --&gt; &lt;setting name=\"cacheEnabled\" value=\"false\"/&gt; &lt;!-- Sets the number of seconds the driver will wait for a response from the database --&gt; &lt;setting name=\"defaultStatementTimeout\" value=\"5\"/&gt; &lt;!-- Enables automatic mapping from classic database column names A_COLUMN to camel case classic Java property names aColumn --&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/&gt; &lt;!-- Allows JDBC support for generated keys. A compatible driver is required. This setting forces generated keys to be used if set to true, as some drivers deny compatibility but still work --&gt; &lt;setting name=\"useGeneratedKeys\" value=\"true\"/&gt; &lt;!-- Allows to support column references --&gt; &lt;setting name=\"useColumnLabel\" value=\"true\"/&gt; &lt;/settings&gt; &lt;!-- Continue editing here --&gt;&lt;/configuration&gt; test// 单元测试 // …","link":"/2019/07/21/数据库实训“活在华工”微信小程序 - 后端实现(spring boot + maven) /"}],"tags":[{"name":"Introduction","slug":"Introduction","link":"/tags/Introduction/"},{"name":"Virtualization","slug":"Virtualization","link":"/tags/Virtualization/"},{"name":"IaaS","slug":"IaaS","link":"/tags/IaaS/"},{"name":"PaaS","slug":"PaaS","link":"/tags/PaaS/"},{"name":"Micro OSes","slug":"Micro-OSes","link":"/tags/Micro-OSes/"},{"name":"Microservices","slug":"Microservices","link":"/tags/Microservices/"},{"name":"Unikernels","slug":"Unikernels","link":"/tags/Unikernels/"},{"name":"Database System","slug":"Database-System","link":"/tags/Database-System/"},{"name":"CI/CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Computer Network","slug":"Computer-Network","link":"/tags/Computer-Network/"},{"name":"GitHub Pages","slug":"GitHub-Pages","link":"/tags/GitHub-Pages/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"sprint boot","slug":"sprint-boot","link":"/tags/sprint-boot/"},{"name":"maven","slug":"maven","link":"/tags/maven/"}],"categories":[{"name":"Cloud Computing","slug":"Cloud-Computing","link":"/categories/Cloud-Computing/"},{"name":"Configuration","slug":"Configuration","link":"/categories/Configuration/"},{"name":"Fullstack","slug":"Fullstack","link":"/categories/Fullstack/"}]}